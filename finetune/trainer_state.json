{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 40.0,
  "eval_steps": 500,
  "global_step": 760,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.052980132450331126,
      "grad_norm": 1.0121660232543945,
      "learning_rate": 0.0,
      "loss": 1.5993,
      "step": 1
    },
    {
      "epoch": 0.10596026490066225,
      "grad_norm": 0.9846355319023132,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.6508,
      "step": 2
    },
    {
      "epoch": 0.15894039735099338,
      "grad_norm": 0.8396814465522766,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.6234,
      "step": 3
    },
    {
      "epoch": 0.2119205298013245,
      "grad_norm": 0.7034152150154114,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.5582,
      "step": 4
    },
    {
      "epoch": 0.26490066225165565,
      "grad_norm": 0.7800838351249695,
      "learning_rate": 0.00011999999999999999,
      "loss": 1.5658,
      "step": 5
    },
    {
      "epoch": 0.31788079470198677,
      "grad_norm": 0.6252052187919617,
      "learning_rate": 0.00015,
      "loss": 1.5449,
      "step": 6
    },
    {
      "epoch": 0.3708609271523179,
      "grad_norm": 0.4720239043235779,
      "learning_rate": 0.00017999999999999998,
      "loss": 1.5062,
      "step": 7
    },
    {
      "epoch": 0.423841059602649,
      "grad_norm": 0.41750144958496094,
      "learning_rate": 0.00020999999999999998,
      "loss": 1.4877,
      "step": 8
    },
    {
      "epoch": 0.4768211920529801,
      "grad_norm": 0.40261536836624146,
      "learning_rate": 0.00023999999999999998,
      "loss": 1.4897,
      "step": 9
    },
    {
      "epoch": 0.5298013245033113,
      "grad_norm": 0.501020610332489,
      "learning_rate": 0.00027,
      "loss": 1.4548,
      "step": 10
    },
    {
      "epoch": 0.5827814569536424,
      "grad_norm": 0.39790892601013184,
      "learning_rate": 0.0003,
      "loss": 1.4652,
      "step": 11
    },
    {
      "epoch": 0.6357615894039735,
      "grad_norm": 0.39769217371940613,
      "learning_rate": 0.00029959999999999996,
      "loss": 1.4118,
      "step": 12
    },
    {
      "epoch": 0.6887417218543046,
      "grad_norm": 0.37702086567878723,
      "learning_rate": 0.00029919999999999995,
      "loss": 1.4057,
      "step": 13
    },
    {
      "epoch": 0.7417218543046358,
      "grad_norm": 0.29183849692344666,
      "learning_rate": 0.0002988,
      "loss": 1.4467,
      "step": 14
    },
    {
      "epoch": 0.7947019867549668,
      "grad_norm": 0.3101714849472046,
      "learning_rate": 0.0002984,
      "loss": 1.4058,
      "step": 15
    },
    {
      "epoch": 0.847682119205298,
      "grad_norm": 0.30405911803245544,
      "learning_rate": 0.000298,
      "loss": 1.3965,
      "step": 16
    },
    {
      "epoch": 0.9006622516556292,
      "grad_norm": 0.26966944336891174,
      "learning_rate": 0.00029759999999999997,
      "loss": 1.4114,
      "step": 17
    },
    {
      "epoch": 0.9536423841059603,
      "grad_norm": 0.289389044046402,
      "learning_rate": 0.00029719999999999996,
      "loss": 1.4016,
      "step": 18
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2587900459766388,
      "learning_rate": 0.00029679999999999995,
      "loss": 1.3739,
      "step": 19
    },
    {
      "epoch": 1.0529801324503312,
      "grad_norm": 0.2757675349712372,
      "learning_rate": 0.0002964,
      "loss": 1.3667,
      "step": 20
    },
    {
      "epoch": 1.1059602649006623,
      "grad_norm": 0.2689838111400604,
      "learning_rate": 0.000296,
      "loss": 1.3016,
      "step": 21
    },
    {
      "epoch": 1.1589403973509933,
      "grad_norm": 0.23103207349777222,
      "learning_rate": 0.0002956,
      "loss": 1.3206,
      "step": 22
    },
    {
      "epoch": 1.2119205298013245,
      "grad_norm": 0.2431846261024475,
      "learning_rate": 0.00029519999999999997,
      "loss": 1.3335,
      "step": 23
    },
    {
      "epoch": 1.2649006622516556,
      "grad_norm": 0.2300577163696289,
      "learning_rate": 0.00029479999999999996,
      "loss": 1.3287,
      "step": 24
    },
    {
      "epoch": 1.3178807947019868,
      "grad_norm": 0.22092747688293457,
      "learning_rate": 0.00029439999999999995,
      "loss": 1.3591,
      "step": 25
    },
    {
      "epoch": 1.370860927152318,
      "grad_norm": 0.22837987542152405,
      "learning_rate": 0.000294,
      "loss": 1.3412,
      "step": 26
    },
    {
      "epoch": 1.423841059602649,
      "grad_norm": 0.21703973412513733,
      "learning_rate": 0.0002936,
      "loss": 1.2915,
      "step": 27
    },
    {
      "epoch": 1.4768211920529801,
      "grad_norm": 0.30435553193092346,
      "learning_rate": 0.00029319999999999997,
      "loss": 1.3237,
      "step": 28
    },
    {
      "epoch": 1.5298013245033113,
      "grad_norm": 0.21218383312225342,
      "learning_rate": 0.00029279999999999996,
      "loss": 1.3146,
      "step": 29
    },
    {
      "epoch": 1.5827814569536423,
      "grad_norm": 0.28875377774238586,
      "learning_rate": 0.0002924,
      "loss": 1.3246,
      "step": 30
    },
    {
      "epoch": 1.6357615894039736,
      "grad_norm": 0.22756274044513702,
      "learning_rate": 0.000292,
      "loss": 1.3065,
      "step": 31
    },
    {
      "epoch": 1.6887417218543046,
      "grad_norm": 0.26036688685417175,
      "learning_rate": 0.0002916,
      "loss": 1.3125,
      "step": 32
    },
    {
      "epoch": 1.7417218543046358,
      "grad_norm": 0.2875819206237793,
      "learning_rate": 0.0002912,
      "loss": 1.3015,
      "step": 33
    },
    {
      "epoch": 1.794701986754967,
      "grad_norm": 0.22362960875034332,
      "learning_rate": 0.00029079999999999997,
      "loss": 1.3127,
      "step": 34
    },
    {
      "epoch": 1.847682119205298,
      "grad_norm": 0.3090140223503113,
      "learning_rate": 0.00029039999999999996,
      "loss": 1.2888,
      "step": 35
    },
    {
      "epoch": 1.9006622516556293,
      "grad_norm": 0.23388387262821198,
      "learning_rate": 0.00029,
      "loss": 1.3264,
      "step": 36
    },
    {
      "epoch": 1.9536423841059603,
      "grad_norm": 0.24404248595237732,
      "learning_rate": 0.0002896,
      "loss": 1.291,
      "step": 37
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.242313414812088,
      "learning_rate": 0.0002892,
      "loss": 1.2787,
      "step": 38
    },
    {
      "epoch": 2.052980132450331,
      "grad_norm": 0.24042625725269318,
      "learning_rate": 0.00028879999999999997,
      "loss": 1.2275,
      "step": 39
    },
    {
      "epoch": 2.1059602649006623,
      "grad_norm": 0.3425874412059784,
      "learning_rate": 0.00028839999999999996,
      "loss": 1.2425,
      "step": 40
    },
    {
      "epoch": 2.1589403973509933,
      "grad_norm": 0.24684976041316986,
      "learning_rate": 0.00028799999999999995,
      "loss": 1.255,
      "step": 41
    },
    {
      "epoch": 2.2119205298013247,
      "grad_norm": 0.23832853138446808,
      "learning_rate": 0.0002876,
      "loss": 1.2178,
      "step": 42
    },
    {
      "epoch": 2.2649006622516556,
      "grad_norm": 0.2346198856830597,
      "learning_rate": 0.0002872,
      "loss": 1.2097,
      "step": 43
    },
    {
      "epoch": 2.3178807947019866,
      "grad_norm": 0.25500303506851196,
      "learning_rate": 0.0002868,
      "loss": 1.2516,
      "step": 44
    },
    {
      "epoch": 2.370860927152318,
      "grad_norm": 0.25034573674201965,
      "learning_rate": 0.00028639999999999997,
      "loss": 1.242,
      "step": 45
    },
    {
      "epoch": 2.423841059602649,
      "grad_norm": 0.2322676181793213,
      "learning_rate": 0.00028599999999999996,
      "loss": 1.2129,
      "step": 46
    },
    {
      "epoch": 2.47682119205298,
      "grad_norm": 0.2524079382419586,
      "learning_rate": 0.00028559999999999995,
      "loss": 1.2264,
      "step": 47
    },
    {
      "epoch": 2.5298013245033113,
      "grad_norm": 0.31412211060523987,
      "learning_rate": 0.0002852,
      "loss": 1.2446,
      "step": 48
    },
    {
      "epoch": 2.5827814569536423,
      "grad_norm": 0.2533778250217438,
      "learning_rate": 0.0002848,
      "loss": 1.1904,
      "step": 49
    },
    {
      "epoch": 2.6357615894039736,
      "grad_norm": 0.2759335935115814,
      "learning_rate": 0.0002844,
      "loss": 1.2518,
      "step": 50
    },
    {
      "epoch": 2.6887417218543046,
      "grad_norm": 0.2768558859825134,
      "learning_rate": 0.00028399999999999996,
      "loss": 1.2221,
      "step": 51
    },
    {
      "epoch": 2.741721854304636,
      "grad_norm": 0.2751195728778839,
      "learning_rate": 0.0002836,
      "loss": 1.2435,
      "step": 52
    },
    {
      "epoch": 2.794701986754967,
      "grad_norm": 0.27465978264808655,
      "learning_rate": 0.00028319999999999994,
      "loss": 1.2141,
      "step": 53
    },
    {
      "epoch": 2.847682119205298,
      "grad_norm": 0.2773783802986145,
      "learning_rate": 0.0002828,
      "loss": 1.2119,
      "step": 54
    },
    {
      "epoch": 2.9006622516556293,
      "grad_norm": 0.25366437435150146,
      "learning_rate": 0.0002824,
      "loss": 1.2375,
      "step": 55
    },
    {
      "epoch": 2.9536423841059603,
      "grad_norm": 0.2595464289188385,
      "learning_rate": 0.00028199999999999997,
      "loss": 1.2279,
      "step": 56
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.28047871589660645,
      "learning_rate": 0.00028159999999999996,
      "loss": 1.2443,
      "step": 57
    },
    {
      "epoch": 3.052980132450331,
      "grad_norm": 0.28628456592559814,
      "learning_rate": 0.0002812,
      "loss": 1.1504,
      "step": 58
    },
    {
      "epoch": 3.1059602649006623,
      "grad_norm": 0.2733418643474579,
      "learning_rate": 0.0002808,
      "loss": 1.1887,
      "step": 59
    },
    {
      "epoch": 3.1589403973509933,
      "grad_norm": 0.2875728905200958,
      "learning_rate": 0.0002804,
      "loss": 1.1843,
      "step": 60
    },
    {
      "epoch": 3.2119205298013247,
      "grad_norm": 0.3589312732219696,
      "learning_rate": 0.00028,
      "loss": 1.1596,
      "step": 61
    },
    {
      "epoch": 3.2649006622516556,
      "grad_norm": 0.27793508768081665,
      "learning_rate": 0.00027959999999999997,
      "loss": 1.1531,
      "step": 62
    },
    {
      "epoch": 3.3178807947019866,
      "grad_norm": 0.29444727301597595,
      "learning_rate": 0.00027919999999999996,
      "loss": 1.1534,
      "step": 63
    },
    {
      "epoch": 3.370860927152318,
      "grad_norm": 0.26593276858329773,
      "learning_rate": 0.0002788,
      "loss": 1.1653,
      "step": 64
    },
    {
      "epoch": 3.423841059602649,
      "grad_norm": 0.2996608316898346,
      "learning_rate": 0.0002784,
      "loss": 1.1512,
      "step": 65
    },
    {
      "epoch": 3.47682119205298,
      "grad_norm": 0.321778804063797,
      "learning_rate": 0.000278,
      "loss": 1.1378,
      "step": 66
    },
    {
      "epoch": 3.5298013245033113,
      "grad_norm": 0.33961889147758484,
      "learning_rate": 0.00027759999999999997,
      "loss": 1.1478,
      "step": 67
    },
    {
      "epoch": 3.5827814569536423,
      "grad_norm": 0.29428985714912415,
      "learning_rate": 0.0002772,
      "loss": 1.1507,
      "step": 68
    },
    {
      "epoch": 3.6357615894039736,
      "grad_norm": 0.3180677592754364,
      "learning_rate": 0.00027679999999999995,
      "loss": 1.1436,
      "step": 69
    },
    {
      "epoch": 3.6887417218543046,
      "grad_norm": 0.3013301193714142,
      "learning_rate": 0.0002764,
      "loss": 1.1494,
      "step": 70
    },
    {
      "epoch": 3.741721854304636,
      "grad_norm": 0.3073435127735138,
      "learning_rate": 0.000276,
      "loss": 1.1458,
      "step": 71
    },
    {
      "epoch": 3.794701986754967,
      "grad_norm": 0.2946528494358063,
      "learning_rate": 0.0002756,
      "loss": 1.1441,
      "step": 72
    },
    {
      "epoch": 3.847682119205298,
      "grad_norm": 0.3537440598011017,
      "learning_rate": 0.00027519999999999997,
      "loss": 1.1388,
      "step": 73
    },
    {
      "epoch": 3.9006622516556293,
      "grad_norm": 0.30134040117263794,
      "learning_rate": 0.0002748,
      "loss": 1.1398,
      "step": 74
    },
    {
      "epoch": 3.9536423841059603,
      "grad_norm": 0.3493988811969757,
      "learning_rate": 0.00027439999999999995,
      "loss": 1.1545,
      "step": 75
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.3825887143611908,
      "learning_rate": 0.000274,
      "loss": 1.1687,
      "step": 76
    },
    {
      "epoch": 4.052980132450331,
      "grad_norm": 0.3895900249481201,
      "learning_rate": 0.0002736,
      "loss": 1.0715,
      "step": 77
    },
    {
      "epoch": 4.105960264900662,
      "grad_norm": 0.3268938660621643,
      "learning_rate": 0.00027319999999999997,
      "loss": 1.0701,
      "step": 78
    },
    {
      "epoch": 4.158940397350993,
      "grad_norm": 0.403273344039917,
      "learning_rate": 0.00027279999999999996,
      "loss": 1.0688,
      "step": 79
    },
    {
      "epoch": 4.211920529801325,
      "grad_norm": 0.4059722125530243,
      "learning_rate": 0.0002724,
      "loss": 1.0823,
      "step": 80
    },
    {
      "epoch": 4.264900662251655,
      "grad_norm": 0.36459314823150635,
      "learning_rate": 0.00027199999999999994,
      "loss": 1.1003,
      "step": 81
    },
    {
      "epoch": 4.317880794701987,
      "grad_norm": 0.42037758231163025,
      "learning_rate": 0.0002716,
      "loss": 1.0482,
      "step": 82
    },
    {
      "epoch": 4.370860927152318,
      "grad_norm": 0.34525296092033386,
      "learning_rate": 0.0002712,
      "loss": 1.0763,
      "step": 83
    },
    {
      "epoch": 4.423841059602649,
      "grad_norm": 0.40704113245010376,
      "learning_rate": 0.00027079999999999997,
      "loss": 1.0714,
      "step": 84
    },
    {
      "epoch": 4.47682119205298,
      "grad_norm": 0.3373869061470032,
      "learning_rate": 0.00027039999999999996,
      "loss": 1.0531,
      "step": 85
    },
    {
      "epoch": 4.529801324503311,
      "grad_norm": 0.41640129685401917,
      "learning_rate": 0.00027,
      "loss": 1.0752,
      "step": 86
    },
    {
      "epoch": 4.582781456953643,
      "grad_norm": 0.34976932406425476,
      "learning_rate": 0.00026959999999999994,
      "loss": 1.0847,
      "step": 87
    },
    {
      "epoch": 4.635761589403973,
      "grad_norm": 0.43673551082611084,
      "learning_rate": 0.0002692,
      "loss": 1.0859,
      "step": 88
    },
    {
      "epoch": 4.688741721854305,
      "grad_norm": 0.3535812795162201,
      "learning_rate": 0.0002688,
      "loss": 1.0753,
      "step": 89
    },
    {
      "epoch": 4.741721854304636,
      "grad_norm": 0.4107186198234558,
      "learning_rate": 0.0002684,
      "loss": 1.0873,
      "step": 90
    },
    {
      "epoch": 4.7947019867549665,
      "grad_norm": 0.3366256356239319,
      "learning_rate": 0.00026799999999999995,
      "loss": 1.0825,
      "step": 91
    },
    {
      "epoch": 4.847682119205298,
      "grad_norm": 0.37253740429878235,
      "learning_rate": 0.0002676,
      "loss": 1.0771,
      "step": 92
    },
    {
      "epoch": 4.900662251655629,
      "grad_norm": 0.3489876091480255,
      "learning_rate": 0.0002672,
      "loss": 1.0842,
      "step": 93
    },
    {
      "epoch": 4.95364238410596,
      "grad_norm": 0.34906792640686035,
      "learning_rate": 0.0002668,
      "loss": 1.0978,
      "step": 94
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.4124215543270111,
      "learning_rate": 0.00026639999999999997,
      "loss": 1.0722,
      "step": 95
    },
    {
      "epoch": 5.052980132450331,
      "grad_norm": 0.3699839115142822,
      "learning_rate": 0.000266,
      "loss": 0.9919,
      "step": 96
    },
    {
      "epoch": 5.105960264900662,
      "grad_norm": 0.4408796429634094,
      "learning_rate": 0.00026559999999999995,
      "loss": 1.0076,
      "step": 97
    },
    {
      "epoch": 5.158940397350993,
      "grad_norm": 0.3722257614135742,
      "learning_rate": 0.0002652,
      "loss": 0.975,
      "step": 98
    },
    {
      "epoch": 5.211920529801325,
      "grad_norm": 0.379029780626297,
      "learning_rate": 0.0002648,
      "loss": 0.9884,
      "step": 99
    },
    {
      "epoch": 5.264900662251655,
      "grad_norm": 0.3964487910270691,
      "learning_rate": 0.0002644,
      "loss": 1.0035,
      "step": 100
    },
    {
      "epoch": 5.317880794701987,
      "grad_norm": 0.38662129640579224,
      "learning_rate": 0.00026399999999999997,
      "loss": 0.996,
      "step": 101
    },
    {
      "epoch": 5.370860927152318,
      "grad_norm": 0.43964684009552,
      "learning_rate": 0.0002636,
      "loss": 0.9899,
      "step": 102
    },
    {
      "epoch": 5.423841059602649,
      "grad_norm": 0.4133533537387848,
      "learning_rate": 0.00026319999999999995,
      "loss": 0.9905,
      "step": 103
    },
    {
      "epoch": 5.47682119205298,
      "grad_norm": 0.38375747203826904,
      "learning_rate": 0.0002628,
      "loss": 0.9963,
      "step": 104
    },
    {
      "epoch": 5.529801324503311,
      "grad_norm": 0.45883867144584656,
      "learning_rate": 0.0002624,
      "loss": 1.0082,
      "step": 105
    },
    {
      "epoch": 5.582781456953643,
      "grad_norm": 0.4482715427875519,
      "learning_rate": 0.00026199999999999997,
      "loss": 1.0016,
      "step": 106
    },
    {
      "epoch": 5.635761589403973,
      "grad_norm": 0.3803153336048126,
      "learning_rate": 0.00026159999999999996,
      "loss": 0.9863,
      "step": 107
    },
    {
      "epoch": 5.688741721854305,
      "grad_norm": 0.38513362407684326,
      "learning_rate": 0.0002612,
      "loss": 0.9784,
      "step": 108
    },
    {
      "epoch": 5.741721854304636,
      "grad_norm": 0.42545345425605774,
      "learning_rate": 0.00026079999999999994,
      "loss": 1.0049,
      "step": 109
    },
    {
      "epoch": 5.7947019867549665,
      "grad_norm": 0.3951016366481781,
      "learning_rate": 0.0002604,
      "loss": 0.9908,
      "step": 110
    },
    {
      "epoch": 5.847682119205298,
      "grad_norm": 0.4694763422012329,
      "learning_rate": 0.00026,
      "loss": 1.0002,
      "step": 111
    },
    {
      "epoch": 5.900662251655629,
      "grad_norm": 0.44256749749183655,
      "learning_rate": 0.00025959999999999997,
      "loss": 0.9984,
      "step": 112
    },
    {
      "epoch": 5.95364238410596,
      "grad_norm": 0.3809235692024231,
      "learning_rate": 0.00025919999999999996,
      "loss": 1.0072,
      "step": 113
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.44723081588745117,
      "learning_rate": 0.0002588,
      "loss": 1.0192,
      "step": 114
    },
    {
      "epoch": 6.052980132450331,
      "grad_norm": 0.5687791705131531,
      "learning_rate": 0.00025839999999999994,
      "loss": 0.9335,
      "step": 115
    },
    {
      "epoch": 6.105960264900662,
      "grad_norm": 0.6155295968055725,
      "learning_rate": 0.000258,
      "loss": 0.9166,
      "step": 116
    },
    {
      "epoch": 6.158940397350993,
      "grad_norm": 0.682884156703949,
      "learning_rate": 0.0002576,
      "loss": 0.9204,
      "step": 117
    },
    {
      "epoch": 6.211920529801325,
      "grad_norm": 0.5703040957450867,
      "learning_rate": 0.00025719999999999996,
      "loss": 0.8834,
      "step": 118
    },
    {
      "epoch": 6.264900662251655,
      "grad_norm": 0.5393761396408081,
      "learning_rate": 0.00025679999999999995,
      "loss": 0.9132,
      "step": 119
    },
    {
      "epoch": 6.317880794701987,
      "grad_norm": 0.48797476291656494,
      "learning_rate": 0.0002564,
      "loss": 0.9079,
      "step": 120
    },
    {
      "epoch": 6.370860927152318,
      "grad_norm": 0.4398116171360016,
      "learning_rate": 0.000256,
      "loss": 0.9055,
      "step": 121
    },
    {
      "epoch": 6.423841059602649,
      "grad_norm": 0.5648915767669678,
      "learning_rate": 0.0002556,
      "loss": 0.8918,
      "step": 122
    },
    {
      "epoch": 6.47682119205298,
      "grad_norm": 0.46413126587867737,
      "learning_rate": 0.00025519999999999997,
      "loss": 0.9132,
      "step": 123
    },
    {
      "epoch": 6.529801324503311,
      "grad_norm": 0.533875048160553,
      "learning_rate": 0.0002548,
      "loss": 0.9029,
      "step": 124
    },
    {
      "epoch": 6.582781456953643,
      "grad_norm": 0.4653029441833496,
      "learning_rate": 0.00025439999999999995,
      "loss": 0.9029,
      "step": 125
    },
    {
      "epoch": 6.635761589403973,
      "grad_norm": 0.5062006115913391,
      "learning_rate": 0.000254,
      "loss": 0.8988,
      "step": 126
    },
    {
      "epoch": 6.688741721854305,
      "grad_norm": 0.47262316942214966,
      "learning_rate": 0.0002536,
      "loss": 0.9167,
      "step": 127
    },
    {
      "epoch": 6.741721854304636,
      "grad_norm": 0.49606823921203613,
      "learning_rate": 0.0002532,
      "loss": 0.9003,
      "step": 128
    },
    {
      "epoch": 6.7947019867549665,
      "grad_norm": 0.5511474609375,
      "learning_rate": 0.00025279999999999996,
      "loss": 0.9234,
      "step": 129
    },
    {
      "epoch": 6.847682119205298,
      "grad_norm": 0.46628454327583313,
      "learning_rate": 0.0002524,
      "loss": 0.9192,
      "step": 130
    },
    {
      "epoch": 6.900662251655629,
      "grad_norm": 0.45227620005607605,
      "learning_rate": 0.00025199999999999995,
      "loss": 0.9122,
      "step": 131
    },
    {
      "epoch": 6.95364238410596,
      "grad_norm": 0.4565655589103699,
      "learning_rate": 0.0002516,
      "loss": 0.9266,
      "step": 132
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.5148344039916992,
      "learning_rate": 0.0002512,
      "loss": 0.925,
      "step": 133
    },
    {
      "epoch": 7.052980132450331,
      "grad_norm": 0.5422753691673279,
      "learning_rate": 0.00025079999999999997,
      "loss": 0.8017,
      "step": 134
    },
    {
      "epoch": 7.105960264900662,
      "grad_norm": 0.6956285238265991,
      "learning_rate": 0.00025039999999999996,
      "loss": 0.8222,
      "step": 135
    },
    {
      "epoch": 7.158940397350993,
      "grad_norm": 0.9015666842460632,
      "learning_rate": 0.00025,
      "loss": 0.838,
      "step": 136
    },
    {
      "epoch": 7.211920529801325,
      "grad_norm": 0.5660105347633362,
      "learning_rate": 0.00024959999999999994,
      "loss": 0.8117,
      "step": 137
    },
    {
      "epoch": 7.264900662251655,
      "grad_norm": 0.6291729211807251,
      "learning_rate": 0.0002492,
      "loss": 0.8173,
      "step": 138
    },
    {
      "epoch": 7.317880794701987,
      "grad_norm": 0.6108444333076477,
      "learning_rate": 0.0002488,
      "loss": 0.8095,
      "step": 139
    },
    {
      "epoch": 7.370860927152318,
      "grad_norm": 0.639379620552063,
      "learning_rate": 0.00024839999999999997,
      "loss": 0.8078,
      "step": 140
    },
    {
      "epoch": 7.423841059602649,
      "grad_norm": 0.6381272077560425,
      "learning_rate": 0.00024799999999999996,
      "loss": 0.8065,
      "step": 141
    },
    {
      "epoch": 7.47682119205298,
      "grad_norm": 0.6928520798683167,
      "learning_rate": 0.0002476,
      "loss": 0.822,
      "step": 142
    },
    {
      "epoch": 7.529801324503311,
      "grad_norm": 0.4915911555290222,
      "learning_rate": 0.0002472,
      "loss": 0.8115,
      "step": 143
    },
    {
      "epoch": 7.582781456953643,
      "grad_norm": 0.7533326148986816,
      "learning_rate": 0.0002468,
      "loss": 0.8338,
      "step": 144
    },
    {
      "epoch": 7.635761589403973,
      "grad_norm": 0.570243775844574,
      "learning_rate": 0.00024639999999999997,
      "loss": 0.8161,
      "step": 145
    },
    {
      "epoch": 7.688741721854305,
      "grad_norm": 0.5584426522254944,
      "learning_rate": 0.00024599999999999996,
      "loss": 0.8223,
      "step": 146
    },
    {
      "epoch": 7.741721854304636,
      "grad_norm": 0.5692318677902222,
      "learning_rate": 0.00024559999999999995,
      "loss": 0.8276,
      "step": 147
    },
    {
      "epoch": 7.7947019867549665,
      "grad_norm": 0.5064476132392883,
      "learning_rate": 0.0002452,
      "loss": 0.8318,
      "step": 148
    },
    {
      "epoch": 7.847682119205298,
      "grad_norm": 0.5504706501960754,
      "learning_rate": 0.0002448,
      "loss": 0.8505,
      "step": 149
    },
    {
      "epoch": 7.900662251655629,
      "grad_norm": 0.5349166393280029,
      "learning_rate": 0.0002444,
      "loss": 0.8223,
      "step": 150
    },
    {
      "epoch": 7.95364238410596,
      "grad_norm": 0.5536704063415527,
      "learning_rate": 0.000244,
      "loss": 0.8443,
      "step": 151
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.626257061958313,
      "learning_rate": 0.00024359999999999999,
      "loss": 0.8305,
      "step": 152
    },
    {
      "epoch": 8.052980132450331,
      "grad_norm": 0.6232635378837585,
      "learning_rate": 0.00024319999999999998,
      "loss": 0.7373,
      "step": 153
    },
    {
      "epoch": 8.105960264900663,
      "grad_norm": 0.6667976379394531,
      "learning_rate": 0.0002428,
      "loss": 0.7138,
      "step": 154
    },
    {
      "epoch": 8.158940397350994,
      "grad_norm": 0.7097410559654236,
      "learning_rate": 0.00024239999999999998,
      "loss": 0.719,
      "step": 155
    },
    {
      "epoch": 8.211920529801324,
      "grad_norm": 0.6664825081825256,
      "learning_rate": 0.00024199999999999997,
      "loss": 0.7227,
      "step": 156
    },
    {
      "epoch": 8.264900662251655,
      "grad_norm": 0.6237784624099731,
      "learning_rate": 0.0002416,
      "loss": 0.7367,
      "step": 157
    },
    {
      "epoch": 8.317880794701987,
      "grad_norm": 0.57935631275177,
      "learning_rate": 0.00024119999999999998,
      "loss": 0.7125,
      "step": 158
    },
    {
      "epoch": 8.370860927152318,
      "grad_norm": 0.5804137587547302,
      "learning_rate": 0.00024079999999999997,
      "loss": 0.722,
      "step": 159
    },
    {
      "epoch": 8.42384105960265,
      "grad_norm": 0.5847333073616028,
      "learning_rate": 0.0002404,
      "loss": 0.7275,
      "step": 160
    },
    {
      "epoch": 8.47682119205298,
      "grad_norm": 0.6258063316345215,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.7329,
      "step": 161
    },
    {
      "epoch": 8.52980132450331,
      "grad_norm": 0.5818699598312378,
      "learning_rate": 0.00023959999999999997,
      "loss": 0.7289,
      "step": 162
    },
    {
      "epoch": 8.582781456953642,
      "grad_norm": 0.5911291837692261,
      "learning_rate": 0.0002392,
      "loss": 0.7213,
      "step": 163
    },
    {
      "epoch": 8.635761589403973,
      "grad_norm": 0.5648245215415955,
      "learning_rate": 0.0002388,
      "loss": 0.7332,
      "step": 164
    },
    {
      "epoch": 8.688741721854305,
      "grad_norm": 0.6116892695426941,
      "learning_rate": 0.00023839999999999997,
      "loss": 0.7404,
      "step": 165
    },
    {
      "epoch": 8.741721854304636,
      "grad_norm": 0.5676860213279724,
      "learning_rate": 0.00023799999999999998,
      "loss": 0.7391,
      "step": 166
    },
    {
      "epoch": 8.794701986754967,
      "grad_norm": 0.5882541537284851,
      "learning_rate": 0.0002376,
      "loss": 0.7401,
      "step": 167
    },
    {
      "epoch": 8.847682119205299,
      "grad_norm": 0.5920687317848206,
      "learning_rate": 0.00023719999999999997,
      "loss": 0.748,
      "step": 168
    },
    {
      "epoch": 8.900662251655628,
      "grad_norm": 0.5742899179458618,
      "learning_rate": 0.00023679999999999998,
      "loss": 0.7485,
      "step": 169
    },
    {
      "epoch": 8.95364238410596,
      "grad_norm": 0.6518798470497131,
      "learning_rate": 0.0002364,
      "loss": 0.7268,
      "step": 170
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.6437079310417175,
      "learning_rate": 0.00023599999999999996,
      "loss": 0.7358,
      "step": 171
    },
    {
      "epoch": 9.052980132450331,
      "grad_norm": 0.7145322561264038,
      "learning_rate": 0.00023559999999999998,
      "loss": 0.6451,
      "step": 172
    },
    {
      "epoch": 9.105960264900663,
      "grad_norm": 0.8130457401275635,
      "learning_rate": 0.0002352,
      "loss": 0.6195,
      "step": 173
    },
    {
      "epoch": 9.158940397350994,
      "grad_norm": 0.8784392476081848,
      "learning_rate": 0.00023479999999999996,
      "loss": 0.6216,
      "step": 174
    },
    {
      "epoch": 9.211920529801324,
      "grad_norm": 0.8715589642524719,
      "learning_rate": 0.00023439999999999998,
      "loss": 0.6235,
      "step": 175
    },
    {
      "epoch": 9.264900662251655,
      "grad_norm": 0.9069486260414124,
      "learning_rate": 0.000234,
      "loss": 0.6206,
      "step": 176
    },
    {
      "epoch": 9.317880794701987,
      "grad_norm": 0.6693320274353027,
      "learning_rate": 0.00023359999999999996,
      "loss": 0.6262,
      "step": 177
    },
    {
      "epoch": 9.370860927152318,
      "grad_norm": 0.7595548629760742,
      "learning_rate": 0.00023319999999999998,
      "loss": 0.6266,
      "step": 178
    },
    {
      "epoch": 9.42384105960265,
      "grad_norm": 0.8345719575881958,
      "learning_rate": 0.0002328,
      "loss": 0.6294,
      "step": 179
    },
    {
      "epoch": 9.47682119205298,
      "grad_norm": 0.8279743790626526,
      "learning_rate": 0.00023239999999999996,
      "loss": 0.6419,
      "step": 180
    },
    {
      "epoch": 9.52980132450331,
      "grad_norm": 0.7168450355529785,
      "learning_rate": 0.00023199999999999997,
      "loss": 0.6506,
      "step": 181
    },
    {
      "epoch": 9.582781456953642,
      "grad_norm": 0.952146053314209,
      "learning_rate": 0.0002316,
      "loss": 0.6278,
      "step": 182
    },
    {
      "epoch": 9.635761589403973,
      "grad_norm": 0.7396436929702759,
      "learning_rate": 0.0002312,
      "loss": 0.6417,
      "step": 183
    },
    {
      "epoch": 9.688741721854305,
      "grad_norm": 0.850422739982605,
      "learning_rate": 0.00023079999999999997,
      "loss": 0.6442,
      "step": 184
    },
    {
      "epoch": 9.741721854304636,
      "grad_norm": 0.7271006107330322,
      "learning_rate": 0.0002304,
      "loss": 0.6455,
      "step": 185
    },
    {
      "epoch": 9.794701986754967,
      "grad_norm": 0.7292206883430481,
      "learning_rate": 0.00023,
      "loss": 0.6352,
      "step": 186
    },
    {
      "epoch": 9.847682119205299,
      "grad_norm": 0.691804051399231,
      "learning_rate": 0.00022959999999999997,
      "loss": 0.647,
      "step": 187
    },
    {
      "epoch": 9.900662251655628,
      "grad_norm": 0.686488687992096,
      "learning_rate": 0.0002292,
      "loss": 0.6496,
      "step": 188
    },
    {
      "epoch": 9.95364238410596,
      "grad_norm": 0.7242103219032288,
      "learning_rate": 0.0002288,
      "loss": 0.6537,
      "step": 189
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.6768389940261841,
      "learning_rate": 0.00022839999999999997,
      "loss": 0.6692,
      "step": 190
    },
    {
      "epoch": 10.052980132450331,
      "grad_norm": 0.7370128631591797,
      "learning_rate": 0.00022799999999999999,
      "loss": 0.5519,
      "step": 191
    },
    {
      "epoch": 10.105960264900663,
      "grad_norm": 0.7432978749275208,
      "learning_rate": 0.0002276,
      "loss": 0.5464,
      "step": 192
    },
    {
      "epoch": 10.158940397350994,
      "grad_norm": 0.9929224252700806,
      "learning_rate": 0.00022719999999999997,
      "loss": 0.5366,
      "step": 193
    },
    {
      "epoch": 10.211920529801324,
      "grad_norm": 0.8228952288627625,
      "learning_rate": 0.00022679999999999998,
      "loss": 0.5378,
      "step": 194
    },
    {
      "epoch": 10.264900662251655,
      "grad_norm": 0.6953744888305664,
      "learning_rate": 0.0002264,
      "loss": 0.5266,
      "step": 195
    },
    {
      "epoch": 10.317880794701987,
      "grad_norm": 0.704017162322998,
      "learning_rate": 0.00022599999999999996,
      "loss": 0.5441,
      "step": 196
    },
    {
      "epoch": 10.370860927152318,
      "grad_norm": 0.7636980414390564,
      "learning_rate": 0.00022559999999999998,
      "loss": 0.5473,
      "step": 197
    },
    {
      "epoch": 10.42384105960265,
      "grad_norm": 0.7692121267318726,
      "learning_rate": 0.0002252,
      "loss": 0.5397,
      "step": 198
    },
    {
      "epoch": 10.47682119205298,
      "grad_norm": 0.7623403668403625,
      "learning_rate": 0.00022479999999999996,
      "loss": 0.5389,
      "step": 199
    },
    {
      "epoch": 10.52980132450331,
      "grad_norm": 0.7228531241416931,
      "learning_rate": 0.00022439999999999998,
      "loss": 0.5376,
      "step": 200
    },
    {
      "epoch": 10.582781456953642,
      "grad_norm": 0.6870794296264648,
      "learning_rate": 0.000224,
      "loss": 0.5463,
      "step": 201
    },
    {
      "epoch": 10.635761589403973,
      "grad_norm": 0.6994684934616089,
      "learning_rate": 0.00022359999999999996,
      "loss": 0.5562,
      "step": 202
    },
    {
      "epoch": 10.688741721854305,
      "grad_norm": 0.7549746632575989,
      "learning_rate": 0.00022319999999999998,
      "loss": 0.5466,
      "step": 203
    },
    {
      "epoch": 10.741721854304636,
      "grad_norm": 0.8626981973648071,
      "learning_rate": 0.0002228,
      "loss": 0.5513,
      "step": 204
    },
    {
      "epoch": 10.794701986754967,
      "grad_norm": 0.8304926156997681,
      "learning_rate": 0.00022239999999999996,
      "loss": 0.5475,
      "step": 205
    },
    {
      "epoch": 10.847682119205299,
      "grad_norm": 0.729045033454895,
      "learning_rate": 0.00022199999999999998,
      "loss": 0.5434,
      "step": 206
    },
    {
      "epoch": 10.900662251655628,
      "grad_norm": 0.9483005404472351,
      "learning_rate": 0.0002216,
      "loss": 0.5571,
      "step": 207
    },
    {
      "epoch": 10.95364238410596,
      "grad_norm": 0.8543258905410767,
      "learning_rate": 0.00022119999999999996,
      "loss": 0.5648,
      "step": 208
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.7995381951332092,
      "learning_rate": 0.00022079999999999997,
      "loss": 0.5786,
      "step": 209
    },
    {
      "epoch": 11.052980132450331,
      "grad_norm": 0.8600409030914307,
      "learning_rate": 0.0002204,
      "loss": 0.4637,
      "step": 210
    },
    {
      "epoch": 11.105960264900663,
      "grad_norm": 0.8472620248794556,
      "learning_rate": 0.00021999999999999995,
      "loss": 0.4543,
      "step": 211
    },
    {
      "epoch": 11.158940397350994,
      "grad_norm": 1.0259960889816284,
      "learning_rate": 0.00021959999999999997,
      "loss": 0.4605,
      "step": 212
    },
    {
      "epoch": 11.211920529801324,
      "grad_norm": 1.1185251474380493,
      "learning_rate": 0.0002192,
      "loss": 0.4569,
      "step": 213
    },
    {
      "epoch": 11.264900662251655,
      "grad_norm": 0.8075813055038452,
      "learning_rate": 0.00021879999999999995,
      "loss": 0.4509,
      "step": 214
    },
    {
      "epoch": 11.317880794701987,
      "grad_norm": 0.7845634818077087,
      "learning_rate": 0.00021839999999999997,
      "loss": 0.4456,
      "step": 215
    },
    {
      "epoch": 11.370860927152318,
      "grad_norm": 0.7890740036964417,
      "learning_rate": 0.00021799999999999999,
      "loss": 0.4514,
      "step": 216
    },
    {
      "epoch": 11.42384105960265,
      "grad_norm": 0.8158496022224426,
      "learning_rate": 0.0002176,
      "loss": 0.4532,
      "step": 217
    },
    {
      "epoch": 11.47682119205298,
      "grad_norm": 0.7394245266914368,
      "learning_rate": 0.00021719999999999997,
      "loss": 0.4469,
      "step": 218
    },
    {
      "epoch": 11.52980132450331,
      "grad_norm": 0.7540697455406189,
      "learning_rate": 0.00021679999999999998,
      "loss": 0.4554,
      "step": 219
    },
    {
      "epoch": 11.582781456953642,
      "grad_norm": 0.7810248136520386,
      "learning_rate": 0.0002164,
      "loss": 0.4599,
      "step": 220
    },
    {
      "epoch": 11.635761589403973,
      "grad_norm": 0.750971794128418,
      "learning_rate": 0.00021599999999999996,
      "loss": 0.449,
      "step": 221
    },
    {
      "epoch": 11.688741721854305,
      "grad_norm": 0.7960292100906372,
      "learning_rate": 0.00021559999999999998,
      "loss": 0.456,
      "step": 222
    },
    {
      "epoch": 11.741721854304636,
      "grad_norm": 0.7917991280555725,
      "learning_rate": 0.0002152,
      "loss": 0.469,
      "step": 223
    },
    {
      "epoch": 11.794701986754967,
      "grad_norm": 0.8125789165496826,
      "learning_rate": 0.00021479999999999996,
      "loss": 0.4639,
      "step": 224
    },
    {
      "epoch": 11.847682119205299,
      "grad_norm": 0.7625162601470947,
      "learning_rate": 0.00021439999999999998,
      "loss": 0.4715,
      "step": 225
    },
    {
      "epoch": 11.900662251655628,
      "grad_norm": 0.8836248517036438,
      "learning_rate": 0.000214,
      "loss": 0.4841,
      "step": 226
    },
    {
      "epoch": 11.95364238410596,
      "grad_norm": 0.7573950886726379,
      "learning_rate": 0.00021359999999999996,
      "loss": 0.4794,
      "step": 227
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.8764806985855103,
      "learning_rate": 0.00021319999999999998,
      "loss": 0.4862,
      "step": 228
    },
    {
      "epoch": 12.052980132450331,
      "grad_norm": 0.915065586566925,
      "learning_rate": 0.0002128,
      "loss": 0.388,
      "step": 229
    },
    {
      "epoch": 12.105960264900663,
      "grad_norm": 0.7366735935211182,
      "learning_rate": 0.00021239999999999996,
      "loss": 0.3713,
      "step": 230
    },
    {
      "epoch": 12.158940397350994,
      "grad_norm": 1.0048226118087769,
      "learning_rate": 0.00021199999999999998,
      "loss": 0.3693,
      "step": 231
    },
    {
      "epoch": 12.211920529801324,
      "grad_norm": 0.9476102590560913,
      "learning_rate": 0.0002116,
      "loss": 0.3588,
      "step": 232
    },
    {
      "epoch": 12.264900662251655,
      "grad_norm": 0.9785149693489075,
      "learning_rate": 0.00021119999999999996,
      "loss": 0.3774,
      "step": 233
    },
    {
      "epoch": 12.317880794701987,
      "grad_norm": 0.798060417175293,
      "learning_rate": 0.00021079999999999997,
      "loss": 0.3817,
      "step": 234
    },
    {
      "epoch": 12.370860927152318,
      "grad_norm": 0.8380874395370483,
      "learning_rate": 0.0002104,
      "loss": 0.3753,
      "step": 235
    },
    {
      "epoch": 12.42384105960265,
      "grad_norm": 0.8855201601982117,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.3756,
      "step": 236
    },
    {
      "epoch": 12.47682119205298,
      "grad_norm": 0.8177995681762695,
      "learning_rate": 0.00020959999999999997,
      "loss": 0.3829,
      "step": 237
    },
    {
      "epoch": 12.52980132450331,
      "grad_norm": 0.8302266597747803,
      "learning_rate": 0.0002092,
      "loss": 0.3751,
      "step": 238
    },
    {
      "epoch": 12.582781456953642,
      "grad_norm": 0.8028110265731812,
      "learning_rate": 0.00020879999999999998,
      "loss": 0.3653,
      "step": 239
    },
    {
      "epoch": 12.635761589403973,
      "grad_norm": 0.8612349629402161,
      "learning_rate": 0.00020839999999999997,
      "loss": 0.3729,
      "step": 240
    },
    {
      "epoch": 12.688741721854305,
      "grad_norm": 0.8649883270263672,
      "learning_rate": 0.000208,
      "loss": 0.3814,
      "step": 241
    },
    {
      "epoch": 12.741721854304636,
      "grad_norm": 0.8071001172065735,
      "learning_rate": 0.00020759999999999998,
      "loss": 0.3939,
      "step": 242
    },
    {
      "epoch": 12.794701986754967,
      "grad_norm": 0.8616245985031128,
      "learning_rate": 0.00020719999999999997,
      "loss": 0.4016,
      "step": 243
    },
    {
      "epoch": 12.847682119205299,
      "grad_norm": 0.8309255242347717,
      "learning_rate": 0.00020679999999999999,
      "loss": 0.405,
      "step": 244
    },
    {
      "epoch": 12.900662251655628,
      "grad_norm": 0.8344518542289734,
      "learning_rate": 0.00020639999999999998,
      "loss": 0.3986,
      "step": 245
    },
    {
      "epoch": 12.95364238410596,
      "grad_norm": 0.8518084287643433,
      "learning_rate": 0.00020599999999999997,
      "loss": 0.4092,
      "step": 246
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.9350442290306091,
      "learning_rate": 0.00020559999999999998,
      "loss": 0.3945,
      "step": 247
    },
    {
      "epoch": 13.052980132450331,
      "grad_norm": 0.8008397817611694,
      "learning_rate": 0.0002052,
      "loss": 0.3036,
      "step": 248
    },
    {
      "epoch": 13.105960264900663,
      "grad_norm": 0.9350380897521973,
      "learning_rate": 0.00020479999999999996,
      "loss": 0.2964,
      "step": 249
    },
    {
      "epoch": 13.158940397350994,
      "grad_norm": 0.9571491479873657,
      "learning_rate": 0.00020439999999999998,
      "loss": 0.3003,
      "step": 250
    },
    {
      "epoch": 13.211920529801324,
      "grad_norm": 0.9563506841659546,
      "learning_rate": 0.000204,
      "loss": 0.3119,
      "step": 251
    },
    {
      "epoch": 13.264900662251655,
      "grad_norm": 0.8892274498939514,
      "learning_rate": 0.00020359999999999996,
      "loss": 0.304,
      "step": 252
    },
    {
      "epoch": 13.317880794701987,
      "grad_norm": 0.9066987037658691,
      "learning_rate": 0.00020319999999999998,
      "loss": 0.3051,
      "step": 253
    },
    {
      "epoch": 13.370860927152318,
      "grad_norm": 0.837742805480957,
      "learning_rate": 0.0002028,
      "loss": 0.2959,
      "step": 254
    },
    {
      "epoch": 13.42384105960265,
      "grad_norm": 0.8945572972297668,
      "learning_rate": 0.0002024,
      "loss": 0.3146,
      "step": 255
    },
    {
      "epoch": 13.47682119205298,
      "grad_norm": 0.8467859625816345,
      "learning_rate": 0.00020199999999999998,
      "loss": 0.3042,
      "step": 256
    },
    {
      "epoch": 13.52980132450331,
      "grad_norm": 0.8628197312355042,
      "learning_rate": 0.0002016,
      "loss": 0.3162,
      "step": 257
    },
    {
      "epoch": 13.582781456953642,
      "grad_norm": 0.8324030041694641,
      "learning_rate": 0.00020119999999999998,
      "loss": 0.3167,
      "step": 258
    },
    {
      "epoch": 13.635761589403973,
      "grad_norm": 0.9326741099357605,
      "learning_rate": 0.00020079999999999997,
      "loss": 0.3277,
      "step": 259
    },
    {
      "epoch": 13.688741721854305,
      "grad_norm": 0.9066693782806396,
      "learning_rate": 0.0002004,
      "loss": 0.3173,
      "step": 260
    },
    {
      "epoch": 13.741721854304636,
      "grad_norm": 0.8076902627944946,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.32,
      "step": 261
    },
    {
      "epoch": 13.794701986754967,
      "grad_norm": 0.9098812341690063,
      "learning_rate": 0.00019959999999999997,
      "loss": 0.3209,
      "step": 262
    },
    {
      "epoch": 13.847682119205299,
      "grad_norm": 0.9091811180114746,
      "learning_rate": 0.0001992,
      "loss": 0.3262,
      "step": 263
    },
    {
      "epoch": 13.900662251655628,
      "grad_norm": 0.8306370377540588,
      "learning_rate": 0.00019879999999999998,
      "loss": 0.329,
      "step": 264
    },
    {
      "epoch": 13.95364238410596,
      "grad_norm": 0.9612171649932861,
      "learning_rate": 0.00019839999999999997,
      "loss": 0.3354,
      "step": 265
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.9676288366317749,
      "learning_rate": 0.000198,
      "loss": 0.3318,
      "step": 266
    },
    {
      "epoch": 14.052980132450331,
      "grad_norm": 0.8296003937721252,
      "learning_rate": 0.00019759999999999998,
      "loss": 0.2511,
      "step": 267
    },
    {
      "epoch": 14.105960264900663,
      "grad_norm": 0.8854679465293884,
      "learning_rate": 0.00019719999999999997,
      "loss": 0.2583,
      "step": 268
    },
    {
      "epoch": 14.158940397350994,
      "grad_norm": 0.9112536907196045,
      "learning_rate": 0.00019679999999999999,
      "loss": 0.2456,
      "step": 269
    },
    {
      "epoch": 14.211920529801324,
      "grad_norm": 0.8713668584823608,
      "learning_rate": 0.00019639999999999998,
      "loss": 0.2417,
      "step": 270
    },
    {
      "epoch": 14.264900662251655,
      "grad_norm": 0.763512134552002,
      "learning_rate": 0.00019599999999999997,
      "loss": 0.2459,
      "step": 271
    },
    {
      "epoch": 14.317880794701987,
      "grad_norm": 0.8678223490715027,
      "learning_rate": 0.00019559999999999998,
      "loss": 0.2531,
      "step": 272
    },
    {
      "epoch": 14.370860927152318,
      "grad_norm": 0.9075247645378113,
      "learning_rate": 0.00019519999999999997,
      "loss": 0.2582,
      "step": 273
    },
    {
      "epoch": 14.42384105960265,
      "grad_norm": 0.8930363059043884,
      "learning_rate": 0.0001948,
      "loss": 0.2506,
      "step": 274
    },
    {
      "epoch": 14.47682119205298,
      "grad_norm": 0.7878338694572449,
      "learning_rate": 0.00019439999999999998,
      "loss": 0.254,
      "step": 275
    },
    {
      "epoch": 14.52980132450331,
      "grad_norm": 0.80744868516922,
      "learning_rate": 0.00019399999999999997,
      "loss": 0.2489,
      "step": 276
    },
    {
      "epoch": 14.582781456953642,
      "grad_norm": 0.86318039894104,
      "learning_rate": 0.0001936,
      "loss": 0.2545,
      "step": 277
    },
    {
      "epoch": 14.635761589403973,
      "grad_norm": 0.7916306853294373,
      "learning_rate": 0.00019319999999999998,
      "loss": 0.2591,
      "step": 278
    },
    {
      "epoch": 14.688741721854305,
      "grad_norm": 0.8877050876617432,
      "learning_rate": 0.0001928,
      "loss": 0.2608,
      "step": 279
    },
    {
      "epoch": 14.741721854304636,
      "grad_norm": 0.8862960934638977,
      "learning_rate": 0.0001924,
      "loss": 0.2538,
      "step": 280
    },
    {
      "epoch": 14.794701986754967,
      "grad_norm": 0.8833765983581543,
      "learning_rate": 0.00019199999999999998,
      "loss": 0.2601,
      "step": 281
    },
    {
      "epoch": 14.847682119205299,
      "grad_norm": 0.9025760889053345,
      "learning_rate": 0.0001916,
      "loss": 0.2613,
      "step": 282
    },
    {
      "epoch": 14.900662251655628,
      "grad_norm": 0.8369347453117371,
      "learning_rate": 0.00019119999999999999,
      "loss": 0.2537,
      "step": 283
    },
    {
      "epoch": 14.95364238410596,
      "grad_norm": 0.8930452466011047,
      "learning_rate": 0.00019079999999999998,
      "loss": 0.2719,
      "step": 284
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.8872690200805664,
      "learning_rate": 0.0001904,
      "loss": 0.2755,
      "step": 285
    },
    {
      "epoch": 15.052980132450331,
      "grad_norm": 0.8240447640419006,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.2035,
      "step": 286
    },
    {
      "epoch": 15.105960264900663,
      "grad_norm": 0.8562732338905334,
      "learning_rate": 0.00018959999999999997,
      "loss": 0.1982,
      "step": 287
    },
    {
      "epoch": 15.158940397350994,
      "grad_norm": 0.8250528573989868,
      "learning_rate": 0.0001892,
      "loss": 0.1944,
      "step": 288
    },
    {
      "epoch": 15.211920529801324,
      "grad_norm": 0.9793153405189514,
      "learning_rate": 0.00018879999999999998,
      "loss": 0.2043,
      "step": 289
    },
    {
      "epoch": 15.264900662251655,
      "grad_norm": 0.856846809387207,
      "learning_rate": 0.00018839999999999997,
      "loss": 0.2035,
      "step": 290
    },
    {
      "epoch": 15.317880794701987,
      "grad_norm": 0.7490712404251099,
      "learning_rate": 0.000188,
      "loss": 0.1972,
      "step": 291
    },
    {
      "epoch": 15.370860927152318,
      "grad_norm": 0.7621092796325684,
      "learning_rate": 0.00018759999999999998,
      "loss": 0.197,
      "step": 292
    },
    {
      "epoch": 15.42384105960265,
      "grad_norm": 0.7961542010307312,
      "learning_rate": 0.0001872,
      "loss": 0.2069,
      "step": 293
    },
    {
      "epoch": 15.47682119205298,
      "grad_norm": 0.847085177898407,
      "learning_rate": 0.0001868,
      "loss": 0.2016,
      "step": 294
    },
    {
      "epoch": 15.52980132450331,
      "grad_norm": 0.8170605897903442,
      "learning_rate": 0.00018639999999999998,
      "loss": 0.2024,
      "step": 295
    },
    {
      "epoch": 15.582781456953642,
      "grad_norm": 0.8172492384910583,
      "learning_rate": 0.000186,
      "loss": 0.2103,
      "step": 296
    },
    {
      "epoch": 15.635761589403973,
      "grad_norm": 0.8335999846458435,
      "learning_rate": 0.00018559999999999998,
      "loss": 0.2077,
      "step": 297
    },
    {
      "epoch": 15.688741721854305,
      "grad_norm": 0.8061791062355042,
      "learning_rate": 0.00018519999999999998,
      "loss": 0.1987,
      "step": 298
    },
    {
      "epoch": 15.741721854304636,
      "grad_norm": 0.851218044757843,
      "learning_rate": 0.0001848,
      "loss": 0.2064,
      "step": 299
    },
    {
      "epoch": 15.794701986754967,
      "grad_norm": 0.8567624092102051,
      "learning_rate": 0.00018439999999999998,
      "loss": 0.2151,
      "step": 300
    },
    {
      "epoch": 15.847682119205299,
      "grad_norm": 0.7983704805374146,
      "learning_rate": 0.00018399999999999997,
      "loss": 0.2119,
      "step": 301
    },
    {
      "epoch": 15.900662251655628,
      "grad_norm": 0.7889737486839294,
      "learning_rate": 0.0001836,
      "loss": 0.2284,
      "step": 302
    },
    {
      "epoch": 15.95364238410596,
      "grad_norm": 0.8165530562400818,
      "learning_rate": 0.00018319999999999998,
      "loss": 0.2195,
      "step": 303
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.8980270028114319,
      "learning_rate": 0.00018279999999999997,
      "loss": 0.2194,
      "step": 304
    },
    {
      "epoch": 16.05298013245033,
      "grad_norm": 0.678528368473053,
      "learning_rate": 0.0001824,
      "loss": 0.1615,
      "step": 305
    },
    {
      "epoch": 16.105960264900663,
      "grad_norm": 0.7348119616508484,
      "learning_rate": 0.00018199999999999998,
      "loss": 0.1562,
      "step": 306
    },
    {
      "epoch": 16.158940397350992,
      "grad_norm": 0.8015395402908325,
      "learning_rate": 0.00018159999999999997,
      "loss": 0.1616,
      "step": 307
    },
    {
      "epoch": 16.211920529801326,
      "grad_norm": 0.7465611696243286,
      "learning_rate": 0.00018119999999999999,
      "loss": 0.1643,
      "step": 308
    },
    {
      "epoch": 16.264900662251655,
      "grad_norm": 0.7535673379898071,
      "learning_rate": 0.00018079999999999998,
      "loss": 0.1564,
      "step": 309
    },
    {
      "epoch": 16.31788079470199,
      "grad_norm": 0.7464054226875305,
      "learning_rate": 0.0001804,
      "loss": 0.1608,
      "step": 310
    },
    {
      "epoch": 16.370860927152318,
      "grad_norm": 0.7643371820449829,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.1608,
      "step": 311
    },
    {
      "epoch": 16.423841059602648,
      "grad_norm": 0.930866003036499,
      "learning_rate": 0.0001796,
      "loss": 0.1639,
      "step": 312
    },
    {
      "epoch": 16.47682119205298,
      "grad_norm": 0.7570917010307312,
      "learning_rate": 0.0001792,
      "loss": 0.1649,
      "step": 313
    },
    {
      "epoch": 16.52980132450331,
      "grad_norm": 0.8103352189064026,
      "learning_rate": 0.00017879999999999998,
      "loss": 0.1682,
      "step": 314
    },
    {
      "epoch": 16.582781456953644,
      "grad_norm": 0.8528102040290833,
      "learning_rate": 0.0001784,
      "loss": 0.1664,
      "step": 315
    },
    {
      "epoch": 16.635761589403973,
      "grad_norm": 0.7755612730979919,
      "learning_rate": 0.000178,
      "loss": 0.1657,
      "step": 316
    },
    {
      "epoch": 16.688741721854306,
      "grad_norm": 0.8132497668266296,
      "learning_rate": 0.00017759999999999998,
      "loss": 0.1687,
      "step": 317
    },
    {
      "epoch": 16.741721854304636,
      "grad_norm": 0.7915125489234924,
      "learning_rate": 0.0001772,
      "loss": 0.1723,
      "step": 318
    },
    {
      "epoch": 16.794701986754966,
      "grad_norm": 0.7840440273284912,
      "learning_rate": 0.0001768,
      "loss": 0.1697,
      "step": 319
    },
    {
      "epoch": 16.8476821192053,
      "grad_norm": 0.8448277115821838,
      "learning_rate": 0.00017639999999999998,
      "loss": 0.1734,
      "step": 320
    },
    {
      "epoch": 16.90066225165563,
      "grad_norm": 0.8313537836074829,
      "learning_rate": 0.000176,
      "loss": 0.1792,
      "step": 321
    },
    {
      "epoch": 16.95364238410596,
      "grad_norm": 0.903390645980835,
      "learning_rate": 0.00017559999999999999,
      "loss": 0.1748,
      "step": 322
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.9214382767677307,
      "learning_rate": 0.00017519999999999998,
      "loss": 0.179,
      "step": 323
    },
    {
      "epoch": 17.05298013245033,
      "grad_norm": 0.6443375945091248,
      "learning_rate": 0.0001748,
      "loss": 0.1246,
      "step": 324
    },
    {
      "epoch": 17.105960264900663,
      "grad_norm": 0.6664726138114929,
      "learning_rate": 0.00017439999999999998,
      "loss": 0.13,
      "step": 325
    },
    {
      "epoch": 17.158940397350992,
      "grad_norm": 0.7753483057022095,
      "learning_rate": 0.00017399999999999997,
      "loss": 0.1252,
      "step": 326
    },
    {
      "epoch": 17.211920529801326,
      "grad_norm": 0.7515293955802917,
      "learning_rate": 0.0001736,
      "loss": 0.1287,
      "step": 327
    },
    {
      "epoch": 17.264900662251655,
      "grad_norm": 0.700289785861969,
      "learning_rate": 0.00017319999999999998,
      "loss": 0.1323,
      "step": 328
    },
    {
      "epoch": 17.31788079470199,
      "grad_norm": 0.7679101824760437,
      "learning_rate": 0.00017279999999999997,
      "loss": 0.1296,
      "step": 329
    },
    {
      "epoch": 17.370860927152318,
      "grad_norm": 0.7133927345275879,
      "learning_rate": 0.0001724,
      "loss": 0.135,
      "step": 330
    },
    {
      "epoch": 17.423841059602648,
      "grad_norm": 0.7054288387298584,
      "learning_rate": 0.000172,
      "loss": 0.1332,
      "step": 331
    },
    {
      "epoch": 17.47682119205298,
      "grad_norm": 0.7720192074775696,
      "learning_rate": 0.00017159999999999997,
      "loss": 0.1369,
      "step": 332
    },
    {
      "epoch": 17.52980132450331,
      "grad_norm": 0.7426086068153381,
      "learning_rate": 0.0001712,
      "loss": 0.1326,
      "step": 333
    },
    {
      "epoch": 17.582781456953644,
      "grad_norm": 0.737982451915741,
      "learning_rate": 0.0001708,
      "loss": 0.1322,
      "step": 334
    },
    {
      "epoch": 17.635761589403973,
      "grad_norm": 0.7394315004348755,
      "learning_rate": 0.00017039999999999997,
      "loss": 0.1364,
      "step": 335
    },
    {
      "epoch": 17.688741721854306,
      "grad_norm": 0.7591524124145508,
      "learning_rate": 0.00016999999999999999,
      "loss": 0.1383,
      "step": 336
    },
    {
      "epoch": 17.741721854304636,
      "grad_norm": 0.7435768246650696,
      "learning_rate": 0.0001696,
      "loss": 0.1375,
      "step": 337
    },
    {
      "epoch": 17.794701986754966,
      "grad_norm": 0.7800264954566956,
      "learning_rate": 0.00016919999999999997,
      "loss": 0.14,
      "step": 338
    },
    {
      "epoch": 17.8476821192053,
      "grad_norm": 0.8282145857810974,
      "learning_rate": 0.00016879999999999998,
      "loss": 0.141,
      "step": 339
    },
    {
      "epoch": 17.90066225165563,
      "grad_norm": 0.7076695561408997,
      "learning_rate": 0.0001684,
      "loss": 0.1411,
      "step": 340
    },
    {
      "epoch": 17.95364238410596,
      "grad_norm": 0.7576806545257568,
      "learning_rate": 0.000168,
      "loss": 0.1456,
      "step": 341
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.8569420576095581,
      "learning_rate": 0.00016759999999999998,
      "loss": 0.1448,
      "step": 342
    },
    {
      "epoch": 18.05298013245033,
      "grad_norm": 0.6658282279968262,
      "learning_rate": 0.0001672,
      "loss": 0.1048,
      "step": 343
    },
    {
      "epoch": 18.105960264900663,
      "grad_norm": 0.7118077278137207,
      "learning_rate": 0.0001668,
      "loss": 0.1077,
      "step": 344
    },
    {
      "epoch": 18.158940397350992,
      "grad_norm": 0.8312075734138489,
      "learning_rate": 0.00016639999999999998,
      "loss": 0.1044,
      "step": 345
    },
    {
      "epoch": 18.211920529801326,
      "grad_norm": 0.6570587158203125,
      "learning_rate": 0.000166,
      "loss": 0.1021,
      "step": 346
    },
    {
      "epoch": 18.264900662251655,
      "grad_norm": 0.6299002170562744,
      "learning_rate": 0.0001656,
      "loss": 0.1049,
      "step": 347
    },
    {
      "epoch": 18.31788079470199,
      "grad_norm": 0.6948242783546448,
      "learning_rate": 0.00016519999999999998,
      "loss": 0.1067,
      "step": 348
    },
    {
      "epoch": 18.370860927152318,
      "grad_norm": 0.6605638265609741,
      "learning_rate": 0.0001648,
      "loss": 0.1063,
      "step": 349
    },
    {
      "epoch": 18.423841059602648,
      "grad_norm": 0.6698749661445618,
      "learning_rate": 0.0001644,
      "loss": 0.1078,
      "step": 350
    },
    {
      "epoch": 18.47682119205298,
      "grad_norm": 0.6822596192359924,
      "learning_rate": 0.00016399999999999997,
      "loss": 0.1075,
      "step": 351
    },
    {
      "epoch": 18.52980132450331,
      "grad_norm": 0.6707020401954651,
      "learning_rate": 0.0001636,
      "loss": 0.11,
      "step": 352
    },
    {
      "epoch": 18.582781456953644,
      "grad_norm": 0.6757182478904724,
      "learning_rate": 0.0001632,
      "loss": 0.1095,
      "step": 353
    },
    {
      "epoch": 18.635761589403973,
      "grad_norm": 0.676990270614624,
      "learning_rate": 0.00016279999999999997,
      "loss": 0.1096,
      "step": 354
    },
    {
      "epoch": 18.688741721854306,
      "grad_norm": 0.66383296251297,
      "learning_rate": 0.0001624,
      "loss": 0.1097,
      "step": 355
    },
    {
      "epoch": 18.741721854304636,
      "grad_norm": 0.6519412994384766,
      "learning_rate": 0.000162,
      "loss": 0.1089,
      "step": 356
    },
    {
      "epoch": 18.794701986754966,
      "grad_norm": 0.6988329291343689,
      "learning_rate": 0.00016159999999999997,
      "loss": 0.1126,
      "step": 357
    },
    {
      "epoch": 18.8476821192053,
      "grad_norm": 0.6934881806373596,
      "learning_rate": 0.0001612,
      "loss": 0.1181,
      "step": 358
    },
    {
      "epoch": 18.90066225165563,
      "grad_norm": 0.9326412677764893,
      "learning_rate": 0.0001608,
      "loss": 0.1165,
      "step": 359
    },
    {
      "epoch": 18.95364238410596,
      "grad_norm": 0.7063689827919006,
      "learning_rate": 0.00016039999999999997,
      "loss": 0.1132,
      "step": 360
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.806907057762146,
      "learning_rate": 0.00015999999999999999,
      "loss": 0.1185,
      "step": 361
    },
    {
      "epoch": 19.05298013245033,
      "grad_norm": 0.5768653154373169,
      "learning_rate": 0.0001596,
      "loss": 0.0876,
      "step": 362
    },
    {
      "epoch": 19.105960264900663,
      "grad_norm": 0.5633402466773987,
      "learning_rate": 0.00015919999999999997,
      "loss": 0.0847,
      "step": 363
    },
    {
      "epoch": 19.158940397350992,
      "grad_norm": 0.679090678691864,
      "learning_rate": 0.00015879999999999998,
      "loss": 0.0865,
      "step": 364
    },
    {
      "epoch": 19.211920529801326,
      "grad_norm": 0.7463622689247131,
      "learning_rate": 0.0001584,
      "loss": 0.0863,
      "step": 365
    },
    {
      "epoch": 19.264900662251655,
      "grad_norm": 0.5902036428451538,
      "learning_rate": 0.00015799999999999996,
      "loss": 0.0845,
      "step": 366
    },
    {
      "epoch": 19.31788079470199,
      "grad_norm": 0.6355366706848145,
      "learning_rate": 0.00015759999999999998,
      "loss": 0.0855,
      "step": 367
    },
    {
      "epoch": 19.370860927152318,
      "grad_norm": 0.6038922667503357,
      "learning_rate": 0.0001572,
      "loss": 0.0917,
      "step": 368
    },
    {
      "epoch": 19.423841059602648,
      "grad_norm": 0.6056914329528809,
      "learning_rate": 0.00015679999999999996,
      "loss": 0.0901,
      "step": 369
    },
    {
      "epoch": 19.47682119205298,
      "grad_norm": 0.6463133692741394,
      "learning_rate": 0.00015639999999999998,
      "loss": 0.0882,
      "step": 370
    },
    {
      "epoch": 19.52980132450331,
      "grad_norm": 0.7143309116363525,
      "learning_rate": 0.000156,
      "loss": 0.0899,
      "step": 371
    },
    {
      "epoch": 19.582781456953644,
      "grad_norm": 0.6572040319442749,
      "learning_rate": 0.00015560000000000001,
      "loss": 0.0929,
      "step": 372
    },
    {
      "epoch": 19.635761589403973,
      "grad_norm": 0.6132667064666748,
      "learning_rate": 0.00015519999999999998,
      "loss": 0.0916,
      "step": 373
    },
    {
      "epoch": 19.688741721854306,
      "grad_norm": 0.6322057247161865,
      "learning_rate": 0.0001548,
      "loss": 0.0917,
      "step": 374
    },
    {
      "epoch": 19.741721854304636,
      "grad_norm": 0.6727322340011597,
      "learning_rate": 0.0001544,
      "loss": 0.0909,
      "step": 375
    },
    {
      "epoch": 19.794701986754966,
      "grad_norm": 0.6494783163070679,
      "learning_rate": 0.00015399999999999998,
      "loss": 0.0912,
      "step": 376
    },
    {
      "epoch": 19.8476821192053,
      "grad_norm": 0.6859469413757324,
      "learning_rate": 0.0001536,
      "loss": 0.0921,
      "step": 377
    },
    {
      "epoch": 19.90066225165563,
      "grad_norm": 0.6137394905090332,
      "learning_rate": 0.0001532,
      "loss": 0.0933,
      "step": 378
    },
    {
      "epoch": 19.95364238410596,
      "grad_norm": 0.7155989408493042,
      "learning_rate": 0.00015279999999999997,
      "loss": 0.0955,
      "step": 379
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.7586011290550232,
      "learning_rate": 0.0001524,
      "loss": 0.097,
      "step": 380
    },
    {
      "epoch": 20.05298013245033,
      "grad_norm": 0.5055949091911316,
      "learning_rate": 0.000152,
      "loss": 0.0742,
      "step": 381
    },
    {
      "epoch": 20.105960264900663,
      "grad_norm": 0.5259211659431458,
      "learning_rate": 0.00015159999999999997,
      "loss": 0.0714,
      "step": 382
    },
    {
      "epoch": 20.158940397350992,
      "grad_norm": 0.6364333629608154,
      "learning_rate": 0.0001512,
      "loss": 0.0691,
      "step": 383
    },
    {
      "epoch": 20.211920529801326,
      "grad_norm": 0.5919306874275208,
      "learning_rate": 0.0001508,
      "loss": 0.0709,
      "step": 384
    },
    {
      "epoch": 20.264900662251655,
      "grad_norm": 0.5247437953948975,
      "learning_rate": 0.00015039999999999997,
      "loss": 0.0732,
      "step": 385
    },
    {
      "epoch": 20.31788079470199,
      "grad_norm": 0.6198199391365051,
      "learning_rate": 0.00015,
      "loss": 0.0688,
      "step": 386
    },
    {
      "epoch": 20.370860927152318,
      "grad_norm": 0.5533562898635864,
      "learning_rate": 0.00014959999999999998,
      "loss": 0.0726,
      "step": 387
    },
    {
      "epoch": 20.423841059602648,
      "grad_norm": 0.5495563745498657,
      "learning_rate": 0.0001492,
      "loss": 0.0752,
      "step": 388
    },
    {
      "epoch": 20.47682119205298,
      "grad_norm": 0.5388818383216858,
      "learning_rate": 0.00014879999999999998,
      "loss": 0.0729,
      "step": 389
    },
    {
      "epoch": 20.52980132450331,
      "grad_norm": 0.6535509824752808,
      "learning_rate": 0.00014839999999999998,
      "loss": 0.0735,
      "step": 390
    },
    {
      "epoch": 20.582781456953644,
      "grad_norm": 0.573354184627533,
      "learning_rate": 0.000148,
      "loss": 0.0748,
      "step": 391
    },
    {
      "epoch": 20.635761589403973,
      "grad_norm": 0.5699065923690796,
      "learning_rate": 0.00014759999999999998,
      "loss": 0.076,
      "step": 392
    },
    {
      "epoch": 20.688741721854306,
      "grad_norm": 0.5986717939376831,
      "learning_rate": 0.00014719999999999997,
      "loss": 0.078,
      "step": 393
    },
    {
      "epoch": 20.741721854304636,
      "grad_norm": 0.5686607956886292,
      "learning_rate": 0.0001468,
      "loss": 0.0747,
      "step": 394
    },
    {
      "epoch": 20.794701986754966,
      "grad_norm": 0.5749548673629761,
      "learning_rate": 0.00014639999999999998,
      "loss": 0.074,
      "step": 395
    },
    {
      "epoch": 20.8476821192053,
      "grad_norm": 0.5656218528747559,
      "learning_rate": 0.000146,
      "loss": 0.0795,
      "step": 396
    },
    {
      "epoch": 20.90066225165563,
      "grad_norm": 0.5950862169265747,
      "learning_rate": 0.0001456,
      "loss": 0.0771,
      "step": 397
    },
    {
      "epoch": 20.95364238410596,
      "grad_norm": 0.6168927550315857,
      "learning_rate": 0.00014519999999999998,
      "loss": 0.0761,
      "step": 398
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.6333736181259155,
      "learning_rate": 0.0001448,
      "loss": 0.0768,
      "step": 399
    },
    {
      "epoch": 21.05298013245033,
      "grad_norm": 0.42947039008140564,
      "learning_rate": 0.00014439999999999999,
      "loss": 0.0586,
      "step": 400
    },
    {
      "epoch": 21.105960264900663,
      "grad_norm": 0.4567084312438965,
      "learning_rate": 0.00014399999999999998,
      "loss": 0.0597,
      "step": 401
    },
    {
      "epoch": 21.158940397350992,
      "grad_norm": 0.44929182529449463,
      "learning_rate": 0.0001436,
      "loss": 0.0595,
      "step": 402
    },
    {
      "epoch": 21.211920529801326,
      "grad_norm": 0.4921175241470337,
      "learning_rate": 0.00014319999999999998,
      "loss": 0.0606,
      "step": 403
    },
    {
      "epoch": 21.264900662251655,
      "grad_norm": 0.47757717967033386,
      "learning_rate": 0.00014279999999999997,
      "loss": 0.0612,
      "step": 404
    },
    {
      "epoch": 21.31788079470199,
      "grad_norm": 0.523349940776825,
      "learning_rate": 0.0001424,
      "loss": 0.0596,
      "step": 405
    },
    {
      "epoch": 21.370860927152318,
      "grad_norm": 0.46230772137641907,
      "learning_rate": 0.00014199999999999998,
      "loss": 0.0617,
      "step": 406
    },
    {
      "epoch": 21.423841059602648,
      "grad_norm": 0.49627819657325745,
      "learning_rate": 0.00014159999999999997,
      "loss": 0.0618,
      "step": 407
    },
    {
      "epoch": 21.47682119205298,
      "grad_norm": 0.4869519770145416,
      "learning_rate": 0.0001412,
      "loss": 0.062,
      "step": 408
    },
    {
      "epoch": 21.52980132450331,
      "grad_norm": 0.5115509033203125,
      "learning_rate": 0.00014079999999999998,
      "loss": 0.0622,
      "step": 409
    },
    {
      "epoch": 21.582781456953644,
      "grad_norm": 0.5074730515480042,
      "learning_rate": 0.0001404,
      "loss": 0.0609,
      "step": 410
    },
    {
      "epoch": 21.635761589403973,
      "grad_norm": 0.47901448607444763,
      "learning_rate": 0.00014,
      "loss": 0.0618,
      "step": 411
    },
    {
      "epoch": 21.688741721854306,
      "grad_norm": 0.4692901074886322,
      "learning_rate": 0.00013959999999999998,
      "loss": 0.0608,
      "step": 412
    },
    {
      "epoch": 21.741721854304636,
      "grad_norm": 0.5047757625579834,
      "learning_rate": 0.0001392,
      "loss": 0.0626,
      "step": 413
    },
    {
      "epoch": 21.794701986754966,
      "grad_norm": 0.4902421832084656,
      "learning_rate": 0.00013879999999999999,
      "loss": 0.0624,
      "step": 414
    },
    {
      "epoch": 21.8476821192053,
      "grad_norm": 0.5306506752967834,
      "learning_rate": 0.00013839999999999998,
      "loss": 0.0636,
      "step": 415
    },
    {
      "epoch": 21.90066225165563,
      "grad_norm": 0.48946574330329895,
      "learning_rate": 0.000138,
      "loss": 0.065,
      "step": 416
    },
    {
      "epoch": 21.95364238410596,
      "grad_norm": 0.5165014266967773,
      "learning_rate": 0.00013759999999999998,
      "loss": 0.0639,
      "step": 417
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.6117261648178101,
      "learning_rate": 0.00013719999999999997,
      "loss": 0.0633,
      "step": 418
    },
    {
      "epoch": 22.05298013245033,
      "grad_norm": 0.3727831542491913,
      "learning_rate": 0.0001368,
      "loss": 0.0497,
      "step": 419
    },
    {
      "epoch": 22.105960264900663,
      "grad_norm": 0.4020829200744629,
      "learning_rate": 0.00013639999999999998,
      "loss": 0.0495,
      "step": 420
    },
    {
      "epoch": 22.158940397350992,
      "grad_norm": 0.3998810350894928,
      "learning_rate": 0.00013599999999999997,
      "loss": 0.0493,
      "step": 421
    },
    {
      "epoch": 22.211920529801326,
      "grad_norm": 0.43054041266441345,
      "learning_rate": 0.0001356,
      "loss": 0.0524,
      "step": 422
    },
    {
      "epoch": 22.264900662251655,
      "grad_norm": 0.401431679725647,
      "learning_rate": 0.00013519999999999998,
      "loss": 0.0502,
      "step": 423
    },
    {
      "epoch": 22.31788079470199,
      "grad_norm": 0.42296817898750305,
      "learning_rate": 0.00013479999999999997,
      "loss": 0.0527,
      "step": 424
    },
    {
      "epoch": 22.370860927152318,
      "grad_norm": 0.43600016832351685,
      "learning_rate": 0.0001344,
      "loss": 0.0531,
      "step": 425
    },
    {
      "epoch": 22.423841059602648,
      "grad_norm": 0.40654996037483215,
      "learning_rate": 0.00013399999999999998,
      "loss": 0.0514,
      "step": 426
    },
    {
      "epoch": 22.47682119205298,
      "grad_norm": 0.4262205958366394,
      "learning_rate": 0.0001336,
      "loss": 0.0516,
      "step": 427
    },
    {
      "epoch": 22.52980132450331,
      "grad_norm": 0.41248297691345215,
      "learning_rate": 0.00013319999999999999,
      "loss": 0.052,
      "step": 428
    },
    {
      "epoch": 22.582781456953644,
      "grad_norm": 0.4327571392059326,
      "learning_rate": 0.00013279999999999998,
      "loss": 0.0508,
      "step": 429
    },
    {
      "epoch": 22.635761589403973,
      "grad_norm": 0.4872293770313263,
      "learning_rate": 0.0001324,
      "loss": 0.0518,
      "step": 430
    },
    {
      "epoch": 22.688741721854306,
      "grad_norm": 0.44844064116477966,
      "learning_rate": 0.00013199999999999998,
      "loss": 0.053,
      "step": 431
    },
    {
      "epoch": 22.741721854304636,
      "grad_norm": 0.40048322081565857,
      "learning_rate": 0.00013159999999999997,
      "loss": 0.0514,
      "step": 432
    },
    {
      "epoch": 22.794701986754966,
      "grad_norm": 0.4666101932525635,
      "learning_rate": 0.0001312,
      "loss": 0.0534,
      "step": 433
    },
    {
      "epoch": 22.8476821192053,
      "grad_norm": 0.4287026524543762,
      "learning_rate": 0.00013079999999999998,
      "loss": 0.0527,
      "step": 434
    },
    {
      "epoch": 22.90066225165563,
      "grad_norm": 0.420773983001709,
      "learning_rate": 0.00013039999999999997,
      "loss": 0.0531,
      "step": 435
    },
    {
      "epoch": 22.95364238410596,
      "grad_norm": 0.46522215008735657,
      "learning_rate": 0.00013,
      "loss": 0.0527,
      "step": 436
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.44678863883018494,
      "learning_rate": 0.00012959999999999998,
      "loss": 0.0534,
      "step": 437
    },
    {
      "epoch": 23.05298013245033,
      "grad_norm": 0.30669984221458435,
      "learning_rate": 0.00012919999999999997,
      "loss": 0.0427,
      "step": 438
    },
    {
      "epoch": 23.105960264900663,
      "grad_norm": 0.3149721920490265,
      "learning_rate": 0.0001288,
      "loss": 0.0418,
      "step": 439
    },
    {
      "epoch": 23.158940397350992,
      "grad_norm": 0.33098170161247253,
      "learning_rate": 0.00012839999999999998,
      "loss": 0.0436,
      "step": 440
    },
    {
      "epoch": 23.211920529801326,
      "grad_norm": 0.35634228587150574,
      "learning_rate": 0.000128,
      "loss": 0.0432,
      "step": 441
    },
    {
      "epoch": 23.264900662251655,
      "grad_norm": 0.3673211336135864,
      "learning_rate": 0.00012759999999999998,
      "loss": 0.0419,
      "step": 442
    },
    {
      "epoch": 23.31788079470199,
      "grad_norm": 0.3424186408519745,
      "learning_rate": 0.00012719999999999997,
      "loss": 0.0439,
      "step": 443
    },
    {
      "epoch": 23.370860927152318,
      "grad_norm": 0.3777506351470947,
      "learning_rate": 0.0001268,
      "loss": 0.0451,
      "step": 444
    },
    {
      "epoch": 23.423841059602648,
      "grad_norm": 0.3497847616672516,
      "learning_rate": 0.00012639999999999998,
      "loss": 0.0435,
      "step": 445
    },
    {
      "epoch": 23.47682119205298,
      "grad_norm": 0.3062635660171509,
      "learning_rate": 0.00012599999999999997,
      "loss": 0.043,
      "step": 446
    },
    {
      "epoch": 23.52980132450331,
      "grad_norm": 0.3261680603027344,
      "learning_rate": 0.0001256,
      "loss": 0.0447,
      "step": 447
    },
    {
      "epoch": 23.582781456953644,
      "grad_norm": 0.37767845392227173,
      "learning_rate": 0.00012519999999999998,
      "loss": 0.0435,
      "step": 448
    },
    {
      "epoch": 23.635761589403973,
      "grad_norm": 0.36079293489456177,
      "learning_rate": 0.00012479999999999997,
      "loss": 0.0457,
      "step": 449
    },
    {
      "epoch": 23.688741721854306,
      "grad_norm": 0.3314027488231659,
      "learning_rate": 0.0001244,
      "loss": 0.0443,
      "step": 450
    },
    {
      "epoch": 23.741721854304636,
      "grad_norm": 0.3450615704059601,
      "learning_rate": 0.00012399999999999998,
      "loss": 0.0449,
      "step": 451
    },
    {
      "epoch": 23.794701986754966,
      "grad_norm": 0.33874982595443726,
      "learning_rate": 0.0001236,
      "loss": 0.0456,
      "step": 452
    },
    {
      "epoch": 23.8476821192053,
      "grad_norm": 0.340203195810318,
      "learning_rate": 0.00012319999999999999,
      "loss": 0.045,
      "step": 453
    },
    {
      "epoch": 23.90066225165563,
      "grad_norm": 0.3320026695728302,
      "learning_rate": 0.00012279999999999998,
      "loss": 0.0469,
      "step": 454
    },
    {
      "epoch": 23.95364238410596,
      "grad_norm": 0.3565688133239746,
      "learning_rate": 0.0001224,
      "loss": 0.0459,
      "step": 455
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.33422601222991943,
      "learning_rate": 0.000122,
      "loss": 0.0447,
      "step": 456
    },
    {
      "epoch": 24.05298013245033,
      "grad_norm": 0.22676773369312286,
      "learning_rate": 0.00012159999999999999,
      "loss": 0.0374,
      "step": 457
    },
    {
      "epoch": 24.105960264900663,
      "grad_norm": 0.23584912717342377,
      "learning_rate": 0.00012119999999999999,
      "loss": 0.0367,
      "step": 458
    },
    {
      "epoch": 24.158940397350992,
      "grad_norm": 0.24592545628547668,
      "learning_rate": 0.0001208,
      "loss": 0.0377,
      "step": 459
    },
    {
      "epoch": 24.211920529801326,
      "grad_norm": 0.26277434825897217,
      "learning_rate": 0.00012039999999999999,
      "loss": 0.0388,
      "step": 460
    },
    {
      "epoch": 24.264900662251655,
      "grad_norm": 0.2881653606891632,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.0379,
      "step": 461
    },
    {
      "epoch": 24.31788079470199,
      "grad_norm": 0.2635074257850647,
      "learning_rate": 0.0001196,
      "loss": 0.0374,
      "step": 462
    },
    {
      "epoch": 24.370860927152318,
      "grad_norm": 0.25726082921028137,
      "learning_rate": 0.00011919999999999998,
      "loss": 0.0376,
      "step": 463
    },
    {
      "epoch": 24.423841059602648,
      "grad_norm": 0.2616827189922333,
      "learning_rate": 0.0001188,
      "loss": 0.0379,
      "step": 464
    },
    {
      "epoch": 24.47682119205298,
      "grad_norm": 0.25327038764953613,
      "learning_rate": 0.00011839999999999999,
      "loss": 0.0373,
      "step": 465
    },
    {
      "epoch": 24.52980132450331,
      "grad_norm": 0.26450178027153015,
      "learning_rate": 0.00011799999999999998,
      "loss": 0.0387,
      "step": 466
    },
    {
      "epoch": 24.582781456953644,
      "grad_norm": 0.2448699027299881,
      "learning_rate": 0.0001176,
      "loss": 0.0388,
      "step": 467
    },
    {
      "epoch": 24.635761589403973,
      "grad_norm": 0.23913151025772095,
      "learning_rate": 0.00011719999999999999,
      "loss": 0.038,
      "step": 468
    },
    {
      "epoch": 24.688741721854306,
      "grad_norm": 0.2766101658344269,
      "learning_rate": 0.00011679999999999998,
      "loss": 0.0392,
      "step": 469
    },
    {
      "epoch": 24.741721854304636,
      "grad_norm": 0.2683160901069641,
      "learning_rate": 0.0001164,
      "loss": 0.0381,
      "step": 470
    },
    {
      "epoch": 24.794701986754966,
      "grad_norm": 0.25659528374671936,
      "learning_rate": 0.00011599999999999999,
      "loss": 0.0394,
      "step": 471
    },
    {
      "epoch": 24.8476821192053,
      "grad_norm": 0.2489733099937439,
      "learning_rate": 0.0001156,
      "loss": 0.0399,
      "step": 472
    },
    {
      "epoch": 24.90066225165563,
      "grad_norm": 0.2680702209472656,
      "learning_rate": 0.0001152,
      "loss": 0.0384,
      "step": 473
    },
    {
      "epoch": 24.95364238410596,
      "grad_norm": 0.2776007354259491,
      "learning_rate": 0.00011479999999999999,
      "loss": 0.0389,
      "step": 474
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.3042183220386505,
      "learning_rate": 0.0001144,
      "loss": 0.038,
      "step": 475
    },
    {
      "epoch": 25.05298013245033,
      "grad_norm": 0.20481760799884796,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.0344,
      "step": 476
    },
    {
      "epoch": 25.105960264900663,
      "grad_norm": 0.20034414529800415,
      "learning_rate": 0.00011359999999999998,
      "loss": 0.0331,
      "step": 477
    },
    {
      "epoch": 25.158940397350992,
      "grad_norm": 0.1900206208229065,
      "learning_rate": 0.0001132,
      "loss": 0.033,
      "step": 478
    },
    {
      "epoch": 25.211920529801326,
      "grad_norm": 0.23052991926670074,
      "learning_rate": 0.00011279999999999999,
      "loss": 0.0333,
      "step": 479
    },
    {
      "epoch": 25.264900662251655,
      "grad_norm": 0.1963283270597458,
      "learning_rate": 0.00011239999999999998,
      "loss": 0.0339,
      "step": 480
    },
    {
      "epoch": 25.31788079470199,
      "grad_norm": 0.29198500514030457,
      "learning_rate": 0.000112,
      "loss": 0.0332,
      "step": 481
    },
    {
      "epoch": 25.370860927152318,
      "grad_norm": 0.22256116569042206,
      "learning_rate": 0.00011159999999999999,
      "loss": 0.0323,
      "step": 482
    },
    {
      "epoch": 25.423841059602648,
      "grad_norm": 0.19785919785499573,
      "learning_rate": 0.00011119999999999998,
      "loss": 0.0334,
      "step": 483
    },
    {
      "epoch": 25.47682119205298,
      "grad_norm": 0.2098534256219864,
      "learning_rate": 0.0001108,
      "loss": 0.0339,
      "step": 484
    },
    {
      "epoch": 25.52980132450331,
      "grad_norm": 0.20645023882389069,
      "learning_rate": 0.00011039999999999999,
      "loss": 0.034,
      "step": 485
    },
    {
      "epoch": 25.582781456953644,
      "grad_norm": 0.2026643604040146,
      "learning_rate": 0.00010999999999999998,
      "loss": 0.0343,
      "step": 486
    },
    {
      "epoch": 25.635761589403973,
      "grad_norm": 0.19954262673854828,
      "learning_rate": 0.0001096,
      "loss": 0.0337,
      "step": 487
    },
    {
      "epoch": 25.688741721854306,
      "grad_norm": 0.19830092787742615,
      "learning_rate": 0.00010919999999999998,
      "loss": 0.0346,
      "step": 488
    },
    {
      "epoch": 25.741721854304636,
      "grad_norm": 0.23117050528526306,
      "learning_rate": 0.0001088,
      "loss": 0.0338,
      "step": 489
    },
    {
      "epoch": 25.794701986754966,
      "grad_norm": 0.21191972494125366,
      "learning_rate": 0.00010839999999999999,
      "loss": 0.0339,
      "step": 490
    },
    {
      "epoch": 25.8476821192053,
      "grad_norm": 0.20400121808052063,
      "learning_rate": 0.00010799999999999998,
      "loss": 0.0343,
      "step": 491
    },
    {
      "epoch": 25.90066225165563,
      "grad_norm": 0.1900574415922165,
      "learning_rate": 0.0001076,
      "loss": 0.0341,
      "step": 492
    },
    {
      "epoch": 25.95364238410596,
      "grad_norm": 0.22225618362426758,
      "learning_rate": 0.00010719999999999999,
      "loss": 0.0343,
      "step": 493
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.20974375307559967,
      "learning_rate": 0.00010679999999999998,
      "loss": 0.0347,
      "step": 494
    },
    {
      "epoch": 26.05298013245033,
      "grad_norm": 0.18105222284793854,
      "learning_rate": 0.0001064,
      "loss": 0.0302,
      "step": 495
    },
    {
      "epoch": 26.105960264900663,
      "grad_norm": 0.15226861834526062,
      "learning_rate": 0.00010599999999999999,
      "loss": 0.0304,
      "step": 496
    },
    {
      "epoch": 26.158940397350992,
      "grad_norm": 0.16490516066551208,
      "learning_rate": 0.00010559999999999998,
      "loss": 0.0308,
      "step": 497
    },
    {
      "epoch": 26.211920529801326,
      "grad_norm": 0.1545981466770172,
      "learning_rate": 0.0001052,
      "loss": 0.0302,
      "step": 498
    },
    {
      "epoch": 26.264900662251655,
      "grad_norm": 0.1576855629682541,
      "learning_rate": 0.00010479999999999999,
      "loss": 0.0295,
      "step": 499
    },
    {
      "epoch": 26.31788079470199,
      "grad_norm": 0.17015527188777924,
      "learning_rate": 0.00010439999999999999,
      "loss": 0.0302,
      "step": 500
    },
    {
      "epoch": 26.370860927152318,
      "grad_norm": 0.16796162724494934,
      "learning_rate": 0.000104,
      "loss": 0.0308,
      "step": 501
    },
    {
      "epoch": 26.423841059602648,
      "grad_norm": 0.17186154425144196,
      "learning_rate": 0.00010359999999999998,
      "loss": 0.0306,
      "step": 502
    },
    {
      "epoch": 26.47682119205298,
      "grad_norm": 0.16292573511600494,
      "learning_rate": 0.00010319999999999999,
      "loss": 0.0311,
      "step": 503
    },
    {
      "epoch": 26.52980132450331,
      "grad_norm": 0.1697009801864624,
      "learning_rate": 0.00010279999999999999,
      "loss": 0.0303,
      "step": 504
    },
    {
      "epoch": 26.582781456953644,
      "grad_norm": 0.16144880652427673,
      "learning_rate": 0.00010239999999999998,
      "loss": 0.0302,
      "step": 505
    },
    {
      "epoch": 26.635761589403973,
      "grad_norm": 0.16688597202301025,
      "learning_rate": 0.000102,
      "loss": 0.0303,
      "step": 506
    },
    {
      "epoch": 26.688741721854306,
      "grad_norm": 0.16252437233924866,
      "learning_rate": 0.00010159999999999999,
      "loss": 0.0304,
      "step": 507
    },
    {
      "epoch": 26.741721854304636,
      "grad_norm": 0.19673657417297363,
      "learning_rate": 0.0001012,
      "loss": 0.0316,
      "step": 508
    },
    {
      "epoch": 26.794701986754966,
      "grad_norm": 0.1749112606048584,
      "learning_rate": 0.0001008,
      "loss": 0.0301,
      "step": 509
    },
    {
      "epoch": 26.8476821192053,
      "grad_norm": 0.15797969698905945,
      "learning_rate": 0.00010039999999999999,
      "loss": 0.0309,
      "step": 510
    },
    {
      "epoch": 26.90066225165563,
      "grad_norm": 0.18563032150268555,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.032,
      "step": 511
    },
    {
      "epoch": 26.95364238410596,
      "grad_norm": 0.19442789256572723,
      "learning_rate": 9.96e-05,
      "loss": 0.0307,
      "step": 512
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.19916711747646332,
      "learning_rate": 9.919999999999999e-05,
      "loss": 0.0316,
      "step": 513
    },
    {
      "epoch": 27.05298013245033,
      "grad_norm": 0.13584962487220764,
      "learning_rate": 9.879999999999999e-05,
      "loss": 0.0284,
      "step": 514
    },
    {
      "epoch": 27.105960264900663,
      "grad_norm": 0.14188890159130096,
      "learning_rate": 9.839999999999999e-05,
      "loss": 0.0277,
      "step": 515
    },
    {
      "epoch": 27.158940397350992,
      "grad_norm": 0.15546324849128723,
      "learning_rate": 9.799999999999998e-05,
      "loss": 0.0278,
      "step": 516
    },
    {
      "epoch": 27.211920529801326,
      "grad_norm": 0.14380759000778198,
      "learning_rate": 9.759999999999999e-05,
      "loss": 0.0286,
      "step": 517
    },
    {
      "epoch": 27.264900662251655,
      "grad_norm": 0.15272653102874756,
      "learning_rate": 9.719999999999999e-05,
      "loss": 0.0279,
      "step": 518
    },
    {
      "epoch": 27.31788079470199,
      "grad_norm": 0.1511368751525879,
      "learning_rate": 9.68e-05,
      "loss": 0.0282,
      "step": 519
    },
    {
      "epoch": 27.370860927152318,
      "grad_norm": 0.1536436229944229,
      "learning_rate": 9.64e-05,
      "loss": 0.0283,
      "step": 520
    },
    {
      "epoch": 27.423841059602648,
      "grad_norm": 0.14783956110477448,
      "learning_rate": 9.599999999999999e-05,
      "loss": 0.0281,
      "step": 521
    },
    {
      "epoch": 27.47682119205298,
      "grad_norm": 0.1546032577753067,
      "learning_rate": 9.559999999999999e-05,
      "loss": 0.0284,
      "step": 522
    },
    {
      "epoch": 27.52980132450331,
      "grad_norm": 0.13963717222213745,
      "learning_rate": 9.52e-05,
      "loss": 0.0281,
      "step": 523
    },
    {
      "epoch": 27.582781456953644,
      "grad_norm": 0.1788361370563507,
      "learning_rate": 9.479999999999999e-05,
      "loss": 0.0286,
      "step": 524
    },
    {
      "epoch": 27.635761589403973,
      "grad_norm": 0.13451813161373138,
      "learning_rate": 9.439999999999999e-05,
      "loss": 0.0276,
      "step": 525
    },
    {
      "epoch": 27.688741721854306,
      "grad_norm": 0.13952307403087616,
      "learning_rate": 9.4e-05,
      "loss": 0.0279,
      "step": 526
    },
    {
      "epoch": 27.741721854304636,
      "grad_norm": 0.17188860476016998,
      "learning_rate": 9.36e-05,
      "loss": 0.0289,
      "step": 527
    },
    {
      "epoch": 27.794701986754966,
      "grad_norm": 0.13138937950134277,
      "learning_rate": 9.319999999999999e-05,
      "loss": 0.0276,
      "step": 528
    },
    {
      "epoch": 27.8476821192053,
      "grad_norm": 0.13612531125545502,
      "learning_rate": 9.279999999999999e-05,
      "loss": 0.028,
      "step": 529
    },
    {
      "epoch": 27.90066225165563,
      "grad_norm": 0.15276691317558289,
      "learning_rate": 9.24e-05,
      "loss": 0.0295,
      "step": 530
    },
    {
      "epoch": 27.95364238410596,
      "grad_norm": 0.14417330920696259,
      "learning_rate": 9.199999999999999e-05,
      "loss": 0.0293,
      "step": 531
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.1670989841222763,
      "learning_rate": 9.159999999999999e-05,
      "loss": 0.0288,
      "step": 532
    },
    {
      "epoch": 28.05298013245033,
      "grad_norm": 0.11519313603639603,
      "learning_rate": 9.12e-05,
      "loss": 0.0264,
      "step": 533
    },
    {
      "epoch": 28.105960264900663,
      "grad_norm": 0.11843746900558472,
      "learning_rate": 9.079999999999998e-05,
      "loss": 0.0261,
      "step": 534
    },
    {
      "epoch": 28.158940397350992,
      "grad_norm": 0.11602355539798737,
      "learning_rate": 9.039999999999999e-05,
      "loss": 0.0261,
      "step": 535
    },
    {
      "epoch": 28.211920529801326,
      "grad_norm": 0.1158258318901062,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.0264,
      "step": 536
    },
    {
      "epoch": 28.264900662251655,
      "grad_norm": 0.12547962367534637,
      "learning_rate": 8.96e-05,
      "loss": 0.0262,
      "step": 537
    },
    {
      "epoch": 28.31788079470199,
      "grad_norm": 0.12104909121990204,
      "learning_rate": 8.92e-05,
      "loss": 0.0256,
      "step": 538
    },
    {
      "epoch": 28.370860927152318,
      "grad_norm": 0.12335869669914246,
      "learning_rate": 8.879999999999999e-05,
      "loss": 0.0259,
      "step": 539
    },
    {
      "epoch": 28.423841059602648,
      "grad_norm": 0.1298777312040329,
      "learning_rate": 8.84e-05,
      "loss": 0.027,
      "step": 540
    },
    {
      "epoch": 28.47682119205298,
      "grad_norm": 0.12180298566818237,
      "learning_rate": 8.8e-05,
      "loss": 0.0262,
      "step": 541
    },
    {
      "epoch": 28.52980132450331,
      "grad_norm": 0.12358913570642471,
      "learning_rate": 8.759999999999999e-05,
      "loss": 0.0273,
      "step": 542
    },
    {
      "epoch": 28.582781456953644,
      "grad_norm": 0.12552905082702637,
      "learning_rate": 8.719999999999999e-05,
      "loss": 0.0265,
      "step": 543
    },
    {
      "epoch": 28.635761589403973,
      "grad_norm": 0.12615767121315002,
      "learning_rate": 8.68e-05,
      "loss": 0.0263,
      "step": 544
    },
    {
      "epoch": 28.688741721854306,
      "grad_norm": 0.12302616238594055,
      "learning_rate": 8.639999999999999e-05,
      "loss": 0.0262,
      "step": 545
    },
    {
      "epoch": 28.741721854304636,
      "grad_norm": 0.12439773231744766,
      "learning_rate": 8.6e-05,
      "loss": 0.0265,
      "step": 546
    },
    {
      "epoch": 28.794701986754966,
      "grad_norm": 0.1377318799495697,
      "learning_rate": 8.56e-05,
      "loss": 0.0271,
      "step": 547
    },
    {
      "epoch": 28.8476821192053,
      "grad_norm": 0.12261682003736496,
      "learning_rate": 8.519999999999998e-05,
      "loss": 0.0266,
      "step": 548
    },
    {
      "epoch": 28.90066225165563,
      "grad_norm": 0.14106281101703644,
      "learning_rate": 8.48e-05,
      "loss": 0.0278,
      "step": 549
    },
    {
      "epoch": 28.95364238410596,
      "grad_norm": 0.1357765942811966,
      "learning_rate": 8.439999999999999e-05,
      "loss": 0.0271,
      "step": 550
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.13312141597270966,
      "learning_rate": 8.4e-05,
      "loss": 0.0275,
      "step": 551
    },
    {
      "epoch": 29.05298013245033,
      "grad_norm": 0.1079547330737114,
      "learning_rate": 8.36e-05,
      "loss": 0.0245,
      "step": 552
    },
    {
      "epoch": 29.105960264900663,
      "grad_norm": 0.11601366847753525,
      "learning_rate": 8.319999999999999e-05,
      "loss": 0.025,
      "step": 553
    },
    {
      "epoch": 29.158940397350992,
      "grad_norm": 0.10764028131961823,
      "learning_rate": 8.28e-05,
      "loss": 0.0248,
      "step": 554
    },
    {
      "epoch": 29.211920529801326,
      "grad_norm": 0.11133330315351486,
      "learning_rate": 8.24e-05,
      "loss": 0.0244,
      "step": 555
    },
    {
      "epoch": 29.264900662251655,
      "grad_norm": 0.12159260362386703,
      "learning_rate": 8.199999999999999e-05,
      "loss": 0.0251,
      "step": 556
    },
    {
      "epoch": 29.31788079470199,
      "grad_norm": 0.14260679483413696,
      "learning_rate": 8.16e-05,
      "loss": 0.0256,
      "step": 557
    },
    {
      "epoch": 29.370860927152318,
      "grad_norm": 0.11756056547164917,
      "learning_rate": 8.12e-05,
      "loss": 0.0246,
      "step": 558
    },
    {
      "epoch": 29.423841059602648,
      "grad_norm": 0.11598511040210724,
      "learning_rate": 8.079999999999999e-05,
      "loss": 0.0251,
      "step": 559
    },
    {
      "epoch": 29.47682119205298,
      "grad_norm": 0.11790475249290466,
      "learning_rate": 8.04e-05,
      "loss": 0.025,
      "step": 560
    },
    {
      "epoch": 29.52980132450331,
      "grad_norm": 0.12170960009098053,
      "learning_rate": 7.999999999999999e-05,
      "loss": 0.0254,
      "step": 561
    },
    {
      "epoch": 29.582781456953644,
      "grad_norm": 0.11939700692892075,
      "learning_rate": 7.959999999999998e-05,
      "loss": 0.0253,
      "step": 562
    },
    {
      "epoch": 29.635761589403973,
      "grad_norm": 0.12318737804889679,
      "learning_rate": 7.92e-05,
      "loss": 0.0259,
      "step": 563
    },
    {
      "epoch": 29.688741721854306,
      "grad_norm": 0.12530308961868286,
      "learning_rate": 7.879999999999999e-05,
      "loss": 0.0262,
      "step": 564
    },
    {
      "epoch": 29.741721854304636,
      "grad_norm": 0.12326700240373611,
      "learning_rate": 7.839999999999998e-05,
      "loss": 0.0263,
      "step": 565
    },
    {
      "epoch": 29.794701986754966,
      "grad_norm": 0.11649838089942932,
      "learning_rate": 7.8e-05,
      "loss": 0.0263,
      "step": 566
    },
    {
      "epoch": 29.8476821192053,
      "grad_norm": 0.13085097074508667,
      "learning_rate": 7.759999999999999e-05,
      "loss": 0.026,
      "step": 567
    },
    {
      "epoch": 29.90066225165563,
      "grad_norm": 0.11632899194955826,
      "learning_rate": 7.72e-05,
      "loss": 0.0252,
      "step": 568
    },
    {
      "epoch": 29.95364238410596,
      "grad_norm": 0.11481066048145294,
      "learning_rate": 7.68e-05,
      "loss": 0.0255,
      "step": 569
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.12197268009185791,
      "learning_rate": 7.639999999999999e-05,
      "loss": 0.0254,
      "step": 570
    },
    {
      "epoch": 30.05298013245033,
      "grad_norm": 0.11133091896772385,
      "learning_rate": 7.6e-05,
      "loss": 0.0243,
      "step": 571
    },
    {
      "epoch": 30.105960264900663,
      "grad_norm": 0.10484300553798676,
      "learning_rate": 7.56e-05,
      "loss": 0.0241,
      "step": 572
    },
    {
      "epoch": 30.158940397350992,
      "grad_norm": 0.10523940622806549,
      "learning_rate": 7.519999999999998e-05,
      "loss": 0.0242,
      "step": 573
    },
    {
      "epoch": 30.211920529801326,
      "grad_norm": 0.09883303195238113,
      "learning_rate": 7.479999999999999e-05,
      "loss": 0.0236,
      "step": 574
    },
    {
      "epoch": 30.264900662251655,
      "grad_norm": 0.10660525411367416,
      "learning_rate": 7.439999999999999e-05,
      "loss": 0.0235,
      "step": 575
    },
    {
      "epoch": 30.31788079470199,
      "grad_norm": 0.11721368879079819,
      "learning_rate": 7.4e-05,
      "loss": 0.0239,
      "step": 576
    },
    {
      "epoch": 30.370860927152318,
      "grad_norm": 0.11118445545434952,
      "learning_rate": 7.359999999999999e-05,
      "loss": 0.0238,
      "step": 577
    },
    {
      "epoch": 30.423841059602648,
      "grad_norm": 0.1125614270567894,
      "learning_rate": 7.319999999999999e-05,
      "loss": 0.0241,
      "step": 578
    },
    {
      "epoch": 30.47682119205298,
      "grad_norm": 0.11074669659137726,
      "learning_rate": 7.28e-05,
      "loss": 0.024,
      "step": 579
    },
    {
      "epoch": 30.52980132450331,
      "grad_norm": 0.11311447620391846,
      "learning_rate": 7.24e-05,
      "loss": 0.0246,
      "step": 580
    },
    {
      "epoch": 30.582781456953644,
      "grad_norm": 0.1115870401263237,
      "learning_rate": 7.199999999999999e-05,
      "loss": 0.0234,
      "step": 581
    },
    {
      "epoch": 30.635761589403973,
      "grad_norm": 0.12143110483884811,
      "learning_rate": 7.159999999999999e-05,
      "loss": 0.0245,
      "step": 582
    },
    {
      "epoch": 30.688741721854306,
      "grad_norm": 0.12065330147743225,
      "learning_rate": 7.12e-05,
      "loss": 0.0252,
      "step": 583
    },
    {
      "epoch": 30.741721854304636,
      "grad_norm": 0.13261723518371582,
      "learning_rate": 7.079999999999999e-05,
      "loss": 0.0255,
      "step": 584
    },
    {
      "epoch": 30.794701986754966,
      "grad_norm": 0.10784118622541428,
      "learning_rate": 7.039999999999999e-05,
      "loss": 0.0244,
      "step": 585
    },
    {
      "epoch": 30.8476821192053,
      "grad_norm": 0.11881495267152786,
      "learning_rate": 7e-05,
      "loss": 0.0248,
      "step": 586
    },
    {
      "epoch": 30.90066225165563,
      "grad_norm": 0.1169484406709671,
      "learning_rate": 6.96e-05,
      "loss": 0.0249,
      "step": 587
    },
    {
      "epoch": 30.95364238410596,
      "grad_norm": 0.11811168491840363,
      "learning_rate": 6.919999999999999e-05,
      "loss": 0.025,
      "step": 588
    },
    {
      "epoch": 31.0,
      "grad_norm": 0.13463549315929413,
      "learning_rate": 6.879999999999999e-05,
      "loss": 0.0252,
      "step": 589
    },
    {
      "epoch": 31.05298013245033,
      "grad_norm": 0.10145412385463715,
      "learning_rate": 6.84e-05,
      "loss": 0.0225,
      "step": 590
    },
    {
      "epoch": 31.105960264900663,
      "grad_norm": 0.10618867725133896,
      "learning_rate": 6.799999999999999e-05,
      "loss": 0.0231,
      "step": 591
    },
    {
      "epoch": 31.158940397350992,
      "grad_norm": 0.10266705602407455,
      "learning_rate": 6.759999999999999e-05,
      "loss": 0.0236,
      "step": 592
    },
    {
      "epoch": 31.211920529801326,
      "grad_norm": 0.10624703764915466,
      "learning_rate": 6.72e-05,
      "loss": 0.0232,
      "step": 593
    },
    {
      "epoch": 31.264900662251655,
      "grad_norm": 0.10439515858888626,
      "learning_rate": 6.68e-05,
      "loss": 0.0242,
      "step": 594
    },
    {
      "epoch": 31.31788079470199,
      "grad_norm": 0.10966730862855911,
      "learning_rate": 6.639999999999999e-05,
      "loss": 0.0234,
      "step": 595
    },
    {
      "epoch": 31.370860927152318,
      "grad_norm": 0.10107677429914474,
      "learning_rate": 6.599999999999999e-05,
      "loss": 0.0234,
      "step": 596
    },
    {
      "epoch": 31.423841059602648,
      "grad_norm": 0.10551165044307709,
      "learning_rate": 6.56e-05,
      "loss": 0.0239,
      "step": 597
    },
    {
      "epoch": 31.47682119205298,
      "grad_norm": 0.11183269321918488,
      "learning_rate": 6.519999999999999e-05,
      "loss": 0.0234,
      "step": 598
    },
    {
      "epoch": 31.52980132450331,
      "grad_norm": 0.10930456221103668,
      "learning_rate": 6.479999999999999e-05,
      "loss": 0.0239,
      "step": 599
    },
    {
      "epoch": 31.582781456953644,
      "grad_norm": 0.10792605578899384,
      "learning_rate": 6.44e-05,
      "loss": 0.0233,
      "step": 600
    },
    {
      "epoch": 31.635761589403973,
      "grad_norm": 0.10832934081554413,
      "learning_rate": 6.4e-05,
      "loss": 0.0236,
      "step": 601
    },
    {
      "epoch": 31.688741721854306,
      "grad_norm": 0.11189093440771103,
      "learning_rate": 6.359999999999999e-05,
      "loss": 0.0234,
      "step": 602
    },
    {
      "epoch": 31.741721854304636,
      "grad_norm": 0.11455897986888885,
      "learning_rate": 6.319999999999999e-05,
      "loss": 0.0235,
      "step": 603
    },
    {
      "epoch": 31.794701986754966,
      "grad_norm": 0.11896482110023499,
      "learning_rate": 6.28e-05,
      "loss": 0.0242,
      "step": 604
    },
    {
      "epoch": 31.8476821192053,
      "grad_norm": 0.10872942209243774,
      "learning_rate": 6.239999999999999e-05,
      "loss": 0.024,
      "step": 605
    },
    {
      "epoch": 31.90066225165563,
      "grad_norm": 0.11104817688465118,
      "learning_rate": 6.199999999999999e-05,
      "loss": 0.0235,
      "step": 606
    },
    {
      "epoch": 31.95364238410596,
      "grad_norm": 0.11338706314563751,
      "learning_rate": 6.159999999999999e-05,
      "loss": 0.0238,
      "step": 607
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.12245327979326248,
      "learning_rate": 6.12e-05,
      "loss": 0.0241,
      "step": 608
    },
    {
      "epoch": 32.05298013245033,
      "grad_norm": 0.09678370505571365,
      "learning_rate": 6.0799999999999994e-05,
      "loss": 0.0224,
      "step": 609
    },
    {
      "epoch": 32.10596026490066,
      "grad_norm": 0.0976690724492073,
      "learning_rate": 6.04e-05,
      "loss": 0.023,
      "step": 610
    },
    {
      "epoch": 32.158940397350996,
      "grad_norm": 0.09937600046396255,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.0225,
      "step": 611
    },
    {
      "epoch": 32.211920529801326,
      "grad_norm": 0.10627725720405579,
      "learning_rate": 5.959999999999999e-05,
      "loss": 0.0223,
      "step": 612
    },
    {
      "epoch": 32.264900662251655,
      "grad_norm": 0.10043594986200333,
      "learning_rate": 5.9199999999999996e-05,
      "loss": 0.023,
      "step": 613
    },
    {
      "epoch": 32.317880794701985,
      "grad_norm": 0.10015416890382767,
      "learning_rate": 5.88e-05,
      "loss": 0.0224,
      "step": 614
    },
    {
      "epoch": 32.370860927152314,
      "grad_norm": 0.1052602156996727,
      "learning_rate": 5.839999999999999e-05,
      "loss": 0.0227,
      "step": 615
    },
    {
      "epoch": 32.42384105960265,
      "grad_norm": 0.10386581718921661,
      "learning_rate": 5.7999999999999994e-05,
      "loss": 0.0227,
      "step": 616
    },
    {
      "epoch": 32.47682119205298,
      "grad_norm": 0.10368696600198746,
      "learning_rate": 5.76e-05,
      "loss": 0.0231,
      "step": 617
    },
    {
      "epoch": 32.52980132450331,
      "grad_norm": 0.10479549318552017,
      "learning_rate": 5.72e-05,
      "loss": 0.0224,
      "step": 618
    },
    {
      "epoch": 32.58278145695364,
      "grad_norm": 0.10209261626005173,
      "learning_rate": 5.679999999999999e-05,
      "loss": 0.0219,
      "step": 619
    },
    {
      "epoch": 32.63576158940398,
      "grad_norm": 0.1105191707611084,
      "learning_rate": 5.6399999999999995e-05,
      "loss": 0.0232,
      "step": 620
    },
    {
      "epoch": 32.688741721854306,
      "grad_norm": 0.11214980483055115,
      "learning_rate": 5.6e-05,
      "loss": 0.0235,
      "step": 621
    },
    {
      "epoch": 32.741721854304636,
      "grad_norm": 0.11946191638708115,
      "learning_rate": 5.559999999999999e-05,
      "loss": 0.0228,
      "step": 622
    },
    {
      "epoch": 32.794701986754966,
      "grad_norm": 0.11030370742082596,
      "learning_rate": 5.519999999999999e-05,
      "loss": 0.0235,
      "step": 623
    },
    {
      "epoch": 32.847682119205295,
      "grad_norm": 0.11995792388916016,
      "learning_rate": 5.48e-05,
      "loss": 0.0233,
      "step": 624
    },
    {
      "epoch": 32.90066225165563,
      "grad_norm": 0.1101996898651123,
      "learning_rate": 5.44e-05,
      "loss": 0.0238,
      "step": 625
    },
    {
      "epoch": 32.95364238410596,
      "grad_norm": 0.11239733546972275,
      "learning_rate": 5.399999999999999e-05,
      "loss": 0.0229,
      "step": 626
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.11875659972429276,
      "learning_rate": 5.3599999999999995e-05,
      "loss": 0.0235,
      "step": 627
    },
    {
      "epoch": 33.05298013245033,
      "grad_norm": 0.09691958874464035,
      "learning_rate": 5.32e-05,
      "loss": 0.0217,
      "step": 628
    },
    {
      "epoch": 33.10596026490066,
      "grad_norm": 0.10066895186901093,
      "learning_rate": 5.279999999999999e-05,
      "loss": 0.0223,
      "step": 629
    },
    {
      "epoch": 33.158940397350996,
      "grad_norm": 0.09935298562049866,
      "learning_rate": 5.239999999999999e-05,
      "loss": 0.0223,
      "step": 630
    },
    {
      "epoch": 33.211920529801326,
      "grad_norm": 0.1087104082107544,
      "learning_rate": 5.2e-05,
      "loss": 0.0225,
      "step": 631
    },
    {
      "epoch": 33.264900662251655,
      "grad_norm": 0.09934880584478378,
      "learning_rate": 5.1599999999999994e-05,
      "loss": 0.0216,
      "step": 632
    },
    {
      "epoch": 33.317880794701985,
      "grad_norm": 0.10223781317472458,
      "learning_rate": 5.119999999999999e-05,
      "loss": 0.0222,
      "step": 633
    },
    {
      "epoch": 33.370860927152314,
      "grad_norm": 0.09336702525615692,
      "learning_rate": 5.0799999999999995e-05,
      "loss": 0.0223,
      "step": 634
    },
    {
      "epoch": 33.42384105960265,
      "grad_norm": 0.09874784201383591,
      "learning_rate": 5.04e-05,
      "loss": 0.0219,
      "step": 635
    },
    {
      "epoch": 33.47682119205298,
      "grad_norm": 0.10404441505670547,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 0.0227,
      "step": 636
    },
    {
      "epoch": 33.52980132450331,
      "grad_norm": 0.10388778895139694,
      "learning_rate": 4.959999999999999e-05,
      "loss": 0.0225,
      "step": 637
    },
    {
      "epoch": 33.58278145695364,
      "grad_norm": 0.10997314006090164,
      "learning_rate": 4.9199999999999997e-05,
      "loss": 0.0217,
      "step": 638
    },
    {
      "epoch": 33.63576158940398,
      "grad_norm": 0.10631414502859116,
      "learning_rate": 4.8799999999999994e-05,
      "loss": 0.0222,
      "step": 639
    },
    {
      "epoch": 33.688741721854306,
      "grad_norm": 0.10913459211587906,
      "learning_rate": 4.84e-05,
      "loss": 0.022,
      "step": 640
    },
    {
      "epoch": 33.741721854304636,
      "grad_norm": 0.10742228478193283,
      "learning_rate": 4.7999999999999994e-05,
      "loss": 0.0227,
      "step": 641
    },
    {
      "epoch": 33.794701986754966,
      "grad_norm": 0.1121826246380806,
      "learning_rate": 4.76e-05,
      "loss": 0.0228,
      "step": 642
    },
    {
      "epoch": 33.847682119205295,
      "grad_norm": 0.10568450391292572,
      "learning_rate": 4.7199999999999995e-05,
      "loss": 0.0224,
      "step": 643
    },
    {
      "epoch": 33.90066225165563,
      "grad_norm": 0.11167270690202713,
      "learning_rate": 4.68e-05,
      "loss": 0.0228,
      "step": 644
    },
    {
      "epoch": 33.95364238410596,
      "grad_norm": 0.11353199183940887,
      "learning_rate": 4.6399999999999996e-05,
      "loss": 0.0228,
      "step": 645
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.12738732993602753,
      "learning_rate": 4.599999999999999e-05,
      "loss": 0.0236,
      "step": 646
    },
    {
      "epoch": 34.05298013245033,
      "grad_norm": 0.09760280698537827,
      "learning_rate": 4.56e-05,
      "loss": 0.0223,
      "step": 647
    },
    {
      "epoch": 34.10596026490066,
      "grad_norm": 0.09887004643678665,
      "learning_rate": 4.5199999999999994e-05,
      "loss": 0.0215,
      "step": 648
    },
    {
      "epoch": 34.158940397350996,
      "grad_norm": 0.09761784970760345,
      "learning_rate": 4.48e-05,
      "loss": 0.0217,
      "step": 649
    },
    {
      "epoch": 34.211920529801326,
      "grad_norm": 0.0936136320233345,
      "learning_rate": 4.4399999999999995e-05,
      "loss": 0.0216,
      "step": 650
    },
    {
      "epoch": 34.264900662251655,
      "grad_norm": 0.09678184241056442,
      "learning_rate": 4.4e-05,
      "loss": 0.0212,
      "step": 651
    },
    {
      "epoch": 34.317880794701985,
      "grad_norm": 0.09915439784526825,
      "learning_rate": 4.3599999999999996e-05,
      "loss": 0.0214,
      "step": 652
    },
    {
      "epoch": 34.370860927152314,
      "grad_norm": 0.09696522355079651,
      "learning_rate": 4.319999999999999e-05,
      "loss": 0.0216,
      "step": 653
    },
    {
      "epoch": 34.42384105960265,
      "grad_norm": 0.10436313599348068,
      "learning_rate": 4.28e-05,
      "loss": 0.0217,
      "step": 654
    },
    {
      "epoch": 34.47682119205298,
      "grad_norm": 0.09618844836950302,
      "learning_rate": 4.24e-05,
      "loss": 0.0219,
      "step": 655
    },
    {
      "epoch": 34.52980132450331,
      "grad_norm": 0.10307694971561432,
      "learning_rate": 4.2e-05,
      "loss": 0.0217,
      "step": 656
    },
    {
      "epoch": 34.58278145695364,
      "grad_norm": 0.10663111507892609,
      "learning_rate": 4.1599999999999995e-05,
      "loss": 0.0225,
      "step": 657
    },
    {
      "epoch": 34.63576158940398,
      "grad_norm": 0.10300234705209732,
      "learning_rate": 4.12e-05,
      "loss": 0.022,
      "step": 658
    },
    {
      "epoch": 34.688741721854306,
      "grad_norm": 0.10714913159608841,
      "learning_rate": 4.08e-05,
      "loss": 0.022,
      "step": 659
    },
    {
      "epoch": 34.741721854304636,
      "grad_norm": 0.10210081189870834,
      "learning_rate": 4.039999999999999e-05,
      "loss": 0.022,
      "step": 660
    },
    {
      "epoch": 34.794701986754966,
      "grad_norm": 0.10664383322000504,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.022,
      "step": 661
    },
    {
      "epoch": 34.847682119205295,
      "grad_norm": 0.11163440346717834,
      "learning_rate": 3.96e-05,
      "loss": 0.0226,
      "step": 662
    },
    {
      "epoch": 34.90066225165563,
      "grad_norm": 0.10453176498413086,
      "learning_rate": 3.919999999999999e-05,
      "loss": 0.0221,
      "step": 663
    },
    {
      "epoch": 34.95364238410596,
      "grad_norm": 0.11611012369394302,
      "learning_rate": 3.8799999999999994e-05,
      "loss": 0.0223,
      "step": 664
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.11639456450939178,
      "learning_rate": 3.84e-05,
      "loss": 0.0223,
      "step": 665
    },
    {
      "epoch": 35.05298013245033,
      "grad_norm": 0.0973225012421608,
      "learning_rate": 3.8e-05,
      "loss": 0.0209,
      "step": 666
    },
    {
      "epoch": 35.10596026490066,
      "grad_norm": 0.10275400429964066,
      "learning_rate": 3.759999999999999e-05,
      "loss": 0.0211,
      "step": 667
    },
    {
      "epoch": 35.158940397350996,
      "grad_norm": 0.09836586564779282,
      "learning_rate": 3.7199999999999996e-05,
      "loss": 0.0213,
      "step": 668
    },
    {
      "epoch": 35.211920529801326,
      "grad_norm": 0.09304116666316986,
      "learning_rate": 3.679999999999999e-05,
      "loss": 0.0211,
      "step": 669
    },
    {
      "epoch": 35.264900662251655,
      "grad_norm": 0.10136054456233978,
      "learning_rate": 3.64e-05,
      "loss": 0.0209,
      "step": 670
    },
    {
      "epoch": 35.317880794701985,
      "grad_norm": 0.10086092352867126,
      "learning_rate": 3.5999999999999994e-05,
      "loss": 0.0218,
      "step": 671
    },
    {
      "epoch": 35.370860927152314,
      "grad_norm": 0.09775989502668381,
      "learning_rate": 3.56e-05,
      "loss": 0.0218,
      "step": 672
    },
    {
      "epoch": 35.42384105960265,
      "grad_norm": 0.09755715727806091,
      "learning_rate": 3.5199999999999995e-05,
      "loss": 0.0215,
      "step": 673
    },
    {
      "epoch": 35.47682119205298,
      "grad_norm": 0.10812124609947205,
      "learning_rate": 3.48e-05,
      "loss": 0.0216,
      "step": 674
    },
    {
      "epoch": 35.52980132450331,
      "grad_norm": 0.10722139477729797,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0213,
      "step": 675
    },
    {
      "epoch": 35.58278145695364,
      "grad_norm": 0.10565869510173798,
      "learning_rate": 3.399999999999999e-05,
      "loss": 0.0213,
      "step": 676
    },
    {
      "epoch": 35.63576158940398,
      "grad_norm": 0.09712250530719757,
      "learning_rate": 3.36e-05,
      "loss": 0.0217,
      "step": 677
    },
    {
      "epoch": 35.688741721854306,
      "grad_norm": 0.10453306138515472,
      "learning_rate": 3.3199999999999994e-05,
      "loss": 0.0218,
      "step": 678
    },
    {
      "epoch": 35.741721854304636,
      "grad_norm": 0.10895140469074249,
      "learning_rate": 3.28e-05,
      "loss": 0.0213,
      "step": 679
    },
    {
      "epoch": 35.794701986754966,
      "grad_norm": 0.09511617571115494,
      "learning_rate": 3.2399999999999995e-05,
      "loss": 0.0217,
      "step": 680
    },
    {
      "epoch": 35.847682119205295,
      "grad_norm": 0.10743816941976547,
      "learning_rate": 3.2e-05,
      "loss": 0.0215,
      "step": 681
    },
    {
      "epoch": 35.90066225165563,
      "grad_norm": 0.10799386352300644,
      "learning_rate": 3.1599999999999996e-05,
      "loss": 0.0221,
      "step": 682
    },
    {
      "epoch": 35.95364238410596,
      "grad_norm": 0.115693598985672,
      "learning_rate": 3.119999999999999e-05,
      "loss": 0.022,
      "step": 683
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.11659225076436996,
      "learning_rate": 3.0799999999999996e-05,
      "loss": 0.0221,
      "step": 684
    },
    {
      "epoch": 36.05298013245033,
      "grad_norm": 0.09306937456130981,
      "learning_rate": 3.0399999999999997e-05,
      "loss": 0.021,
      "step": 685
    },
    {
      "epoch": 36.10596026490066,
      "grad_norm": 0.09961167722940445,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.0213,
      "step": 686
    },
    {
      "epoch": 36.158940397350996,
      "grad_norm": 0.09812774509191513,
      "learning_rate": 2.9599999999999998e-05,
      "loss": 0.0208,
      "step": 687
    },
    {
      "epoch": 36.211920529801326,
      "grad_norm": 0.09236836433410645,
      "learning_rate": 2.9199999999999995e-05,
      "loss": 0.0221,
      "step": 688
    },
    {
      "epoch": 36.264900662251655,
      "grad_norm": 0.10189761221408844,
      "learning_rate": 2.88e-05,
      "loss": 0.0212,
      "step": 689
    },
    {
      "epoch": 36.317880794701985,
      "grad_norm": 0.09946800768375397,
      "learning_rate": 2.8399999999999996e-05,
      "loss": 0.0209,
      "step": 690
    },
    {
      "epoch": 36.370860927152314,
      "grad_norm": 0.09896794706583023,
      "learning_rate": 2.8e-05,
      "loss": 0.0207,
      "step": 691
    },
    {
      "epoch": 36.42384105960265,
      "grad_norm": 0.10186859220266342,
      "learning_rate": 2.7599999999999997e-05,
      "loss": 0.0213,
      "step": 692
    },
    {
      "epoch": 36.47682119205298,
      "grad_norm": 0.09896797686815262,
      "learning_rate": 2.72e-05,
      "loss": 0.0206,
      "step": 693
    },
    {
      "epoch": 36.52980132450331,
      "grad_norm": 0.10373518615961075,
      "learning_rate": 2.6799999999999998e-05,
      "loss": 0.0213,
      "step": 694
    },
    {
      "epoch": 36.58278145695364,
      "grad_norm": 0.09933996200561523,
      "learning_rate": 2.6399999999999995e-05,
      "loss": 0.0212,
      "step": 695
    },
    {
      "epoch": 36.63576158940398,
      "grad_norm": 0.10007050633430481,
      "learning_rate": 2.6e-05,
      "loss": 0.0209,
      "step": 696
    },
    {
      "epoch": 36.688741721854306,
      "grad_norm": 0.10415304452180862,
      "learning_rate": 2.5599999999999995e-05,
      "loss": 0.0213,
      "step": 697
    },
    {
      "epoch": 36.741721854304636,
      "grad_norm": 0.10296393185853958,
      "learning_rate": 2.52e-05,
      "loss": 0.0219,
      "step": 698
    },
    {
      "epoch": 36.794701986754966,
      "grad_norm": 0.11478380113840103,
      "learning_rate": 2.4799999999999996e-05,
      "loss": 0.0212,
      "step": 699
    },
    {
      "epoch": 36.847682119205295,
      "grad_norm": 0.10592659562826157,
      "learning_rate": 2.4399999999999997e-05,
      "loss": 0.0214,
      "step": 700
    },
    {
      "epoch": 36.90066225165563,
      "grad_norm": 0.10237106680870056,
      "learning_rate": 2.3999999999999997e-05,
      "loss": 0.021,
      "step": 701
    },
    {
      "epoch": 36.95364238410596,
      "grad_norm": 0.1066025048494339,
      "learning_rate": 2.3599999999999998e-05,
      "loss": 0.0213,
      "step": 702
    },
    {
      "epoch": 37.0,
      "grad_norm": 0.11646901071071625,
      "learning_rate": 2.3199999999999998e-05,
      "loss": 0.0214,
      "step": 703
    },
    {
      "epoch": 37.05298013245033,
      "grad_norm": 0.09547687321901321,
      "learning_rate": 2.28e-05,
      "loss": 0.0203,
      "step": 704
    },
    {
      "epoch": 37.10596026490066,
      "grad_norm": 0.09204143285751343,
      "learning_rate": 2.24e-05,
      "loss": 0.0213,
      "step": 705
    },
    {
      "epoch": 37.158940397350996,
      "grad_norm": 0.09024793654680252,
      "learning_rate": 2.2e-05,
      "loss": 0.0209,
      "step": 706
    },
    {
      "epoch": 37.211920529801326,
      "grad_norm": 0.0997285470366478,
      "learning_rate": 2.1599999999999996e-05,
      "loss": 0.0207,
      "step": 707
    },
    {
      "epoch": 37.264900662251655,
      "grad_norm": 0.09419216215610504,
      "learning_rate": 2.12e-05,
      "loss": 0.0207,
      "step": 708
    },
    {
      "epoch": 37.317880794701985,
      "grad_norm": 0.09530315548181534,
      "learning_rate": 2.0799999999999997e-05,
      "loss": 0.0207,
      "step": 709
    },
    {
      "epoch": 37.370860927152314,
      "grad_norm": 0.0968012735247612,
      "learning_rate": 2.04e-05,
      "loss": 0.0212,
      "step": 710
    },
    {
      "epoch": 37.42384105960265,
      "grad_norm": 0.09527431428432465,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.0213,
      "step": 711
    },
    {
      "epoch": 37.47682119205298,
      "grad_norm": 0.09300809353590012,
      "learning_rate": 1.9599999999999995e-05,
      "loss": 0.0212,
      "step": 712
    },
    {
      "epoch": 37.52980132450331,
      "grad_norm": 0.10676093399524689,
      "learning_rate": 1.92e-05,
      "loss": 0.0206,
      "step": 713
    },
    {
      "epoch": 37.58278145695364,
      "grad_norm": 0.09808708727359772,
      "learning_rate": 1.8799999999999996e-05,
      "loss": 0.0203,
      "step": 714
    },
    {
      "epoch": 37.63576158940398,
      "grad_norm": 0.10225364565849304,
      "learning_rate": 1.8399999999999997e-05,
      "loss": 0.021,
      "step": 715
    },
    {
      "epoch": 37.688741721854306,
      "grad_norm": 0.10152173787355423,
      "learning_rate": 1.7999999999999997e-05,
      "loss": 0.0214,
      "step": 716
    },
    {
      "epoch": 37.741721854304636,
      "grad_norm": 0.10079266130924225,
      "learning_rate": 1.7599999999999998e-05,
      "loss": 0.021,
      "step": 717
    },
    {
      "epoch": 37.794701986754966,
      "grad_norm": 0.09966336190700531,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0212,
      "step": 718
    },
    {
      "epoch": 37.847682119205295,
      "grad_norm": 0.09702066332101822,
      "learning_rate": 1.68e-05,
      "loss": 0.0207,
      "step": 719
    },
    {
      "epoch": 37.90066225165563,
      "grad_norm": 0.09995563328266144,
      "learning_rate": 1.64e-05,
      "loss": 0.0205,
      "step": 720
    },
    {
      "epoch": 37.95364238410596,
      "grad_norm": 0.11109748482704163,
      "learning_rate": 1.6e-05,
      "loss": 0.0211,
      "step": 721
    },
    {
      "epoch": 38.0,
      "grad_norm": 0.1232558861374855,
      "learning_rate": 1.5599999999999996e-05,
      "loss": 0.021,
      "step": 722
    },
    {
      "epoch": 38.05298013245033,
      "grad_norm": 0.09550368040800095,
      "learning_rate": 1.5199999999999998e-05,
      "loss": 0.0205,
      "step": 723
    },
    {
      "epoch": 38.10596026490066,
      "grad_norm": 0.10078474134206772,
      "learning_rate": 1.4799999999999999e-05,
      "loss": 0.0204,
      "step": 724
    },
    {
      "epoch": 38.158940397350996,
      "grad_norm": 0.09427047520875931,
      "learning_rate": 1.44e-05,
      "loss": 0.0202,
      "step": 725
    },
    {
      "epoch": 38.211920529801326,
      "grad_norm": 0.09564647078514099,
      "learning_rate": 1.4e-05,
      "loss": 0.0212,
      "step": 726
    },
    {
      "epoch": 38.264900662251655,
      "grad_norm": 0.09176496416330338,
      "learning_rate": 1.36e-05,
      "loss": 0.0211,
      "step": 727
    },
    {
      "epoch": 38.317880794701985,
      "grad_norm": 0.09503936022520065,
      "learning_rate": 1.3199999999999997e-05,
      "loss": 0.0202,
      "step": 728
    },
    {
      "epoch": 38.370860927152314,
      "grad_norm": 0.09393052011728287,
      "learning_rate": 1.2799999999999998e-05,
      "loss": 0.0208,
      "step": 729
    },
    {
      "epoch": 38.42384105960265,
      "grad_norm": 0.09606780111789703,
      "learning_rate": 1.2399999999999998e-05,
      "loss": 0.0206,
      "step": 730
    },
    {
      "epoch": 38.47682119205298,
      "grad_norm": 0.09770569205284119,
      "learning_rate": 1.1999999999999999e-05,
      "loss": 0.0208,
      "step": 731
    },
    {
      "epoch": 38.52980132450331,
      "grad_norm": 0.09976527094841003,
      "learning_rate": 1.1599999999999999e-05,
      "loss": 0.0205,
      "step": 732
    },
    {
      "epoch": 38.58278145695364,
      "grad_norm": 0.10698094964027405,
      "learning_rate": 1.12e-05,
      "loss": 0.0209,
      "step": 733
    },
    {
      "epoch": 38.63576158940398,
      "grad_norm": 0.09905897080898285,
      "learning_rate": 1.0799999999999998e-05,
      "loss": 0.0206,
      "step": 734
    },
    {
      "epoch": 38.688741721854306,
      "grad_norm": 0.098853699862957,
      "learning_rate": 1.0399999999999999e-05,
      "loss": 0.0209,
      "step": 735
    },
    {
      "epoch": 38.741721854304636,
      "grad_norm": 0.099683478474617,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.0213,
      "step": 736
    },
    {
      "epoch": 38.794701986754966,
      "grad_norm": 0.10145287960767746,
      "learning_rate": 9.6e-06,
      "loss": 0.0206,
      "step": 737
    },
    {
      "epoch": 38.847682119205295,
      "grad_norm": 0.10221594572067261,
      "learning_rate": 9.199999999999998e-06,
      "loss": 0.021,
      "step": 738
    },
    {
      "epoch": 38.90066225165563,
      "grad_norm": 0.10277605056762695,
      "learning_rate": 8.799999999999999e-06,
      "loss": 0.0207,
      "step": 739
    },
    {
      "epoch": 38.95364238410596,
      "grad_norm": 0.09929648041725159,
      "learning_rate": 8.4e-06,
      "loss": 0.0205,
      "step": 740
    },
    {
      "epoch": 39.0,
      "grad_norm": 0.10933281481266022,
      "learning_rate": 8e-06,
      "loss": 0.0208,
      "step": 741
    },
    {
      "epoch": 39.05298013245033,
      "grad_norm": 0.09835164248943329,
      "learning_rate": 7.599999999999999e-06,
      "loss": 0.0211,
      "step": 742
    },
    {
      "epoch": 39.10596026490066,
      "grad_norm": 0.09172143042087555,
      "learning_rate": 7.2e-06,
      "loss": 0.0206,
      "step": 743
    },
    {
      "epoch": 39.158940397350996,
      "grad_norm": 0.0906074196100235,
      "learning_rate": 6.8e-06,
      "loss": 0.0201,
      "step": 744
    },
    {
      "epoch": 39.211920529801326,
      "grad_norm": 0.09716484695672989,
      "learning_rate": 6.399999999999999e-06,
      "loss": 0.0207,
      "step": 745
    },
    {
      "epoch": 39.264900662251655,
      "grad_norm": 0.09421961009502411,
      "learning_rate": 5.999999999999999e-06,
      "loss": 0.0205,
      "step": 746
    },
    {
      "epoch": 39.317880794701985,
      "grad_norm": 0.09115114063024521,
      "learning_rate": 5.6e-06,
      "loss": 0.0202,
      "step": 747
    },
    {
      "epoch": 39.370860927152314,
      "grad_norm": 0.1014098972082138,
      "learning_rate": 5.199999999999999e-06,
      "loss": 0.0203,
      "step": 748
    },
    {
      "epoch": 39.42384105960265,
      "grad_norm": 0.08960580080747604,
      "learning_rate": 4.8e-06,
      "loss": 0.02,
      "step": 749
    },
    {
      "epoch": 39.47682119205298,
      "grad_norm": 0.09556297212839127,
      "learning_rate": 4.399999999999999e-06,
      "loss": 0.0202,
      "step": 750
    },
    {
      "epoch": 39.52980132450331,
      "grad_norm": 0.09357039630413055,
      "learning_rate": 4e-06,
      "loss": 0.0201,
      "step": 751
    },
    {
      "epoch": 39.58278145695364,
      "grad_norm": 0.09783431887626648,
      "learning_rate": 3.6e-06,
      "loss": 0.0207,
      "step": 752
    },
    {
      "epoch": 39.63576158940398,
      "grad_norm": 0.09748078137636185,
      "learning_rate": 3.1999999999999994e-06,
      "loss": 0.0205,
      "step": 753
    },
    {
      "epoch": 39.688741721854306,
      "grad_norm": 0.09774455428123474,
      "learning_rate": 2.8e-06,
      "loss": 0.0204,
      "step": 754
    },
    {
      "epoch": 39.741721854304636,
      "grad_norm": 0.09639579802751541,
      "learning_rate": 2.4e-06,
      "loss": 0.021,
      "step": 755
    },
    {
      "epoch": 39.794701986754966,
      "grad_norm": 0.09714775532484055,
      "learning_rate": 2e-06,
      "loss": 0.0208,
      "step": 756
    },
    {
      "epoch": 39.847682119205295,
      "grad_norm": 0.09600167721509933,
      "learning_rate": 1.5999999999999997e-06,
      "loss": 0.0202,
      "step": 757
    },
    {
      "epoch": 39.90066225165563,
      "grad_norm": 0.0951332077383995,
      "learning_rate": 1.2e-06,
      "loss": 0.0209,
      "step": 758
    },
    {
      "epoch": 39.95364238410596,
      "grad_norm": 0.098619744181633,
      "learning_rate": 7.999999999999999e-07,
      "loss": 0.0205,
      "step": 759
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.10980348289012909,
      "learning_rate": 3.9999999999999993e-07,
      "loss": 0.0205,
      "step": 760
    }
  ],
  "logging_steps": 1,
  "max_steps": 760,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.780080804021985e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
